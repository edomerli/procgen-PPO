{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook useful to train using GPUs on Kaggle/Colab, contains the same code of the repo, just reformatted for imports between files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install procgen\n",
    "# !pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "import gym \n",
    "import gym.wrappers\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"Seed all sources of randomness for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionsDataset(Dataset):\n",
    "    def __init__(self, transitions, transform=None, normalize_v_targets=False, v_mu=None, v_std=None):\n",
    "        self.transitions = transitions\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.normalize_v_targets = normalize_v_targets\n",
    "        if normalize_v_targets:\n",
    "            self.v_mu = v_mu\n",
    "            self.v_std = v_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transitions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        state_t = self.transitions[idx]['s_t']\n",
    "        action_t = self.transitions[idx]['a_t']\n",
    "        advantage_t = self.transitions[idx]['A_t']\n",
    "        v_target_t = self.transitions[idx]['v_target_t']\n",
    "\n",
    "        if self.transform:\n",
    "            state_t = self.transform(state_t)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            v_target_t = (v_target_t - self.v_mu) / max(self.v_std, 1e-6)\n",
    "\n",
    "        return state_t, action_t, advantage_t, v_target_t.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, batch_norm):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        if batch_norm:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ImpalaNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_actions, batch_norm):\n",
    "        super(ImpalaNetwork, self).__init__()\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.stems = nn.ModuleList()\n",
    "        self.res_blocks1 = nn.ModuleList()\n",
    "        self.res_blocks2 = nn.ModuleList()\n",
    "\n",
    "        hidden_channels = [32, 64, 64]\n",
    "\n",
    "        for out_channels in hidden_channels:\n",
    "\n",
    "            # Don't use batch_norm in the first layer as it should go after MaxPool2d, \n",
    "            # but it's already present in the successive ConvBlock\n",
    "            self.stems.append(torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=\"same\"),\n",
    "                torch.nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ))\n",
    "\n",
    "            self.res_blocks1.append(torch.nn.Sequential(\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "            ))\n",
    "\n",
    "            self.res_blocks2.append(torch.nn.Sequential(\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "            ))\n",
    "\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_channels[-1] * 7 * 7, out_features=256)\n",
    "\n",
    "        self.out = torch.nn.Linear(256, num_actions)\n",
    "\n",
    "        \n",
    "        if num_actions > 1:\n",
    "            # policy network initialization\n",
    "            nn.init.orthogonal_(self.fc.weight, gain=0.01)\n",
    "            nn.init.constant_(self.fc.bias, 0)\n",
    "        else:\n",
    "            # value network initialization\n",
    "            nn.init.orthogonal_(self.out.weight, gain=1)\n",
    "            nn.init.constant_(self.out.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for stem, res_block1, res_block2 in zip(self.stems, self.res_blocks1, self.res_blocks2):\n",
    "            x = stem(x)\n",
    "            x = res_block1(x) + x\n",
    "            x = res_block2(x) + x\n",
    "\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        if self.num_actions > 1:\n",
    "            logits = self.out(x)\n",
    "            output = torch.distributions.Categorical(logits=logits)\n",
    "        else:\n",
    "            output = self.out(x).squeeze()\n",
    "\n",
    "        return output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, config):\n",
    "        self.policy_net = ImpalaNetwork(config.stack_size * 3, env.action_space.n, config.batch_norm)\n",
    "        self.value_net = ImpalaNetwork(config.stack_size * 3, 1, config.batch_norm)\n",
    "\n",
    "        self.normalize_v_targets = config.normalize_v_targets\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            self.value_mean = 0\n",
    "            self.value_std = 1\n",
    "            self.values_count = 0\n",
    "\n",
    "    # act(), value() and act_and_v() are used during play, hence a single value (.item()) is returned\n",
    "    def act(self, state):\n",
    "        dist = self.policy_net(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action.item()\n",
    "    \n",
    "    def value(self, state):\n",
    "        value = self.value_net(state)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            # denormalize value\n",
    "            value = value * max(self.value_std, 1e-6) + self.value_mean\n",
    "\n",
    "        return value.item()\n",
    "\n",
    "    def act_and_v(self, state):\n",
    "        action = self.act(state)\n",
    "        value = self.value(state)\n",
    "\n",
    "        return action, value\n",
    "    \n",
    "    # actions_dist() and actions_dist_and_v() are used during training, hence the full distributions and values are returned\n",
    "    def actions_dist(self, state):\n",
    "        return self.policy_net(state)\n",
    "    \n",
    "    def actions_dist_and_v(self, state):\n",
    "        dist = self.policy_net(state)\n",
    "        value = self.value_net(state)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            # denormalize value\n",
    "            value = value * max(self.value_std, 1e-6) + self.value_mean\n",
    "\n",
    "        return dist, value\n",
    "      \n",
    "    def to(self, device):\n",
    "        self.policy_net.to(device)\n",
    "        self.value_net.to(device)\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "\n",
    "    def update_v_target_stats(self, v_targets):\n",
    "        self.value_mean = (self.value_mean * self.values_count + v_targets.mean() * len(v_targets)) / (self.values_count + len(v_targets) + 1e-6)\n",
    "        self.value_std = (self.value_std * self.values_count + v_targets.std() * len(v_targets)) / (self.values_count + len(v_targets) + 1e-6)\n",
    "        self.values_count += len(v_targets)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.policy_net.state_dict()\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.policy_net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecorderWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, episode_frequency_rec):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.episode_frequency_rec = episode_frequency_rec\n",
    "\n",
    "        self.episode_counter = 1\n",
    "        self.recording = False if episode_frequency_rec > 1 else True\n",
    "        self.frames = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        if self.recording:\n",
    "            self.frames.append(np.moveaxis(obs, -1, 0))\n",
    "            \n",
    "            if terminated:\n",
    "                self.save_video()\n",
    "                self.recording = False\n",
    "                self.frames = []\n",
    "        \n",
    "        if terminated:\n",
    "            self.episode_counter += 1\n",
    "            if self.episode_counter % self.episode_frequency_rec == 0:\n",
    "                self.recording = True\n",
    "            \n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def save_video(self):\n",
    "        global global_step\n",
    "        wandb.log({\"video\": wandb.Video(np.array(self.frames), caption=f\"step: {global_step} - episode: {self.episode_counter}\", fps=30, format=\"mp4\")})\n",
    "\n",
    "    def close(self):\n",
    "        super().close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, policy_old, train_dataloader, optimizer_policy, optimizer_value, device, config, scheduler_policy=None, scheduler_value=None):\n",
    "\n",
    "    global global_batch\n",
    "\n",
    "    policy.train()\n",
    "    policy_old.eval()\n",
    "    assert not policy_old.policy_net.training and not policy_old.value_net.training, \"Old policy should be in evaluation mode here\"\n",
    "    assert policy.policy_net.training and policy.value_net.training, \"Policy should be in training mode here\"\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for batch, (states, actions, advantages, value_targets) in enumerate(train_dataloader):\n",
    "            # normalize advantages between 0 and 1\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            states = states.to(device)\n",
    "            actions = actions.to(device)\n",
    "            advantages = advantages.to(device)\n",
    "            value_targets = value_targets.to(device)\n",
    "            \n",
    "            dists, values = policy.actions_dist_and_v(states)\n",
    "            old_dists = policy_old.actions_dist(states)\n",
    "\n",
    "            log_probs = dists.log_prob(actions)\n",
    "            old_log_probs = old_dists.log_prob(actions)\n",
    "\n",
    "            # Equivalent of doing exp(log_probs) / exp(old_log_probs) \n",
    "            # but avoids overflows and division by (potentially if underflown) zero, breaking loss function\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "            # clipped surrogate loss\n",
    "            l_clips = -torch.min(ratios * advantages, torch.clip(ratios, 1-config.eps_clip, 1+config.eps_clip) * advantages)\n",
    "            loss_pi = torch.mean(l_clips)\n",
    "            loss_entropy = dists.entropy().mean()\n",
    "            loss_policy = loss_pi - config.entropy_bonus * loss_entropy\n",
    "\n",
    "            # mse loss\n",
    "            loss_value = torch.nn.functional.mse_loss(values, value_targets)\n",
    "\n",
    "            # with two different optimizers\n",
    "            loss_policy.backward()\n",
    "            optimizer_policy.step()\n",
    "            optimizer_policy.zero_grad()\n",
    "\n",
    "            loss_value.backward()\n",
    "            optimizer_value.step()\n",
    "            optimizer_value.zero_grad()\n",
    "\n",
    "            if global_batch % config.log_frequency == 0:\n",
    "                wandb.log({\"train/loss_pi\": loss_pi, \n",
    "                           \"train/loss_v\": loss_value,\n",
    "                           \"train/entropy\": loss_entropy,\n",
    "                           \"train/lr_policy\": optimizer_policy.param_groups[0]['lr'],\n",
    "                           \"train/lr_value\": optimizer_value.param_groups[0]['lr'],\n",
    "                           \"train/batch\": global_batch})\n",
    "            \n",
    "            global_batch += 1\n",
    "        \n",
    "        if scheduler_policy is not None:\n",
    "            scheduler_policy.step()\n",
    "        if scheduler_value is not None:\n",
    "            scheduler_value.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # KL divergence between old and new policy for early stopping\n",
    "            kl_div = torch.distributions.kl.kl_divergence(old_dists, dists).mean().item()\n",
    "            wandb.log({\"train/kl_div\": kl_div, \"train/batch\": global_batch})\n",
    "            if kl_div > config.kl_limit:\n",
    "                print(f\"Early stopping at epoch {epoch} due to KL divergence {round(kl_div, 4)} > {config.kl_limit}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_to_tensor = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "def compute_advantages(values, rewards, gamma, lambda_):\n",
    "    assert len(values) >= 2, \"Values should have at least 2 elements.\"\n",
    "    assert len(values) == len(rewards) + 1, \"Values and rewards should have the same length, with values having one more element.\"\n",
    "    # GAE estimator\n",
    "    deltas = np.array(rewards) + gamma * np.array(values[1:]) - np.array(values[:-1])\n",
    "    advantages = [deltas[-1]] \n",
    "\n",
    "    for t in range(len(deltas)-2, -1, -1):\n",
    "        advantage_t = deltas[t] + gamma * lambda_ * advantages[-1]\n",
    "        advantages.append(advantage_t)\n",
    "\n",
    "    advantages = advantages[::-1]\n",
    "    return advantages\n",
    "\n",
    "def compute_value_targets(advantages, values, rewards, config):\n",
    "    value_targets = []\n",
    "    if config.v_target == \"TD-lambda\":\n",
    "        for t in range(len(advantages)):\n",
    "            value_targets.append(advantages[t] + values[t])\n",
    "    elif config.v_target == \"MC\":\n",
    "        value_targets.append(rewards[-1])\n",
    "        for t in range(len(rewards)-2, -1, -1):\n",
    "            value_targets.append(rewards[t] + config.gamma * value_targets[-1])\n",
    "        value_targets = value_targets[::-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value target type {config.v_target}, choose between 'TD-lambda' and 'MC'.\")\n",
    "    return value_targets\n",
    "\n",
    "\n",
    "def play_and_train(env, env_test, policy, policy_old, optimizer_policy, optimizer_value, device, config, **kwargs):  \n",
    "\n",
    "    global global_step\n",
    "\n",
    "    for iteration in range(config.num_iterations):\n",
    "        print(f\"===============Iteration {iteration+1}===============\")\n",
    "        print(f\"Playing {config.iteration_timesteps} steps...\")\n",
    "\n",
    "        transitions = []\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        # stack frames together to introduce temporal information\n",
    "        state_deque = deque()\n",
    "        for _ in range(config.stack_size):\n",
    "            state_deque.append(frame_to_tensor(obs))\n",
    "\n",
    "        state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "        trajectory = {\n",
    "            'states': [state],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'values': [],\n",
    "        }\n",
    "\n",
    "        policy.eval()\n",
    "\n",
    "        for step in tqdm(range(config.iteration_timesteps)):\n",
    "            assert not policy.policy_net.training and not policy.value_net.training, \"Policy should be in evaluation mode here\"\n",
    "\n",
    "            state = state.unsqueeze(0).to(device)\n",
    "            action, value = policy.act_and_v(state)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            truncated = truncated or step == config.iteration_timesteps - 1\n",
    "\n",
    "            # update step count\n",
    "            global_step += 1\n",
    "\n",
    "            # collect transition info in trajectory\n",
    "            trajectory['values'].append(value)\n",
    "\n",
    "            trajectory['actions'].append(action)\n",
    "            trajectory['rewards'].append(reward)\n",
    "\n",
    "            # udpate state to become next state using the new observation\n",
    "            state_deque.popleft()\n",
    "            state_deque.append(frame_to_tensor(next_obs))\n",
    "            state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "            trajectory['states'].append(state)\n",
    "\n",
    "\n",
    "            if terminated or truncated:\n",
    "                # see terminated vs truncated API at https://farama.org/Gymnasium-Terminated-Truncated-Step-API\n",
    "                if terminated:\n",
    "                    # final value is 0 if the episode terminated, i.e. reached a final state\n",
    "                    trajectory['values'].append(0)\n",
    "                else:\n",
    "                    # bootstrap if the episode was truncated, i.e. didn't reach a final state\n",
    "                    state = state.unsqueeze(0).to(device)\n",
    "                    value = policy.value(state)\n",
    "                    trajectory['values'].append(value)\n",
    "                \n",
    "                advantages = compute_advantages(trajectory['values'], trajectory['rewards'], config.gamma, config.lambda_)\n",
    "\n",
    "                value_targets = compute_value_targets(advantages, trajectory['values'], trajectory['rewards'], config)\n",
    "\n",
    "                if config.normalize_v_targets:\n",
    "                    policy.update_v_target_stats(np.array(value_targets))\n",
    "\n",
    "\n",
    "                # convert trajectory into list of transitions\n",
    "                for t in range(len(trajectory['states'])-1):    # -1 because advantages already encode the value of state t+1\n",
    "                    transitions.append({\n",
    "                        's_t': trajectory['states'][t],\n",
    "                        'a_t': trajectory['actions'][t],\n",
    "                        'A_t': advantages[t],\n",
    "                        'v_target_t': value_targets[t],\n",
    "                    })\n",
    "\n",
    "                # log and update episodes count only if episode terminated\n",
    "                if terminated:\n",
    "                    wandb.log({\"play/episodic_reward\": sum(trajectory['rewards']), \n",
    "                            \"play/episode_length\": len(trajectory['states'])-1,\n",
    "                            \"play/step\": global_step})\n",
    "                \n",
    "                if step < config.iteration_timesteps - 1:\n",
    "                    # reset env and trajectory\n",
    "                    obs, _ = env.reset()\n",
    "\n",
    "                    state_deque = deque()\n",
    "                    for _ in range(config.stack_size):\n",
    "                        state_deque.append(frame_to_tensor(obs))\n",
    "\n",
    "                    state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "                    trajectory = {\n",
    "                        'states': [state],\n",
    "                        'actions': [],\n",
    "                        'rewards': [],\n",
    "                        'values': [],\n",
    "                    }\n",
    "\n",
    "\n",
    "        # end of play loop\n",
    "        if config.normalize_v_targets:\n",
    "            dataset = TransitionsDataset(transitions, normalize_v_targets=True, v_mu=policy.value_mean, v_std=policy.value_std)\n",
    "        else:\n",
    "            dataset = TransitionsDataset(transitions)\n",
    "        train_dataloader = DataLoader(dataset, \n",
    "                                    batch_size=config.batch_size, \n",
    "                                    shuffle=True)\n",
    "\n",
    "        print(f\"Collected {len(transitions)} transitions, starting training...\")\n",
    "\n",
    "        ### TRAIN LOOP ###\n",
    "        train(policy, policy_old, train_dataloader, optimizer_policy, optimizer_value, device, config, **kwargs)\n",
    "        print(\"Training done!\")\n",
    "\n",
    "        del policy_old\n",
    "        policy_old = copy.deepcopy(policy)\n",
    "        policy_old.to(device)\n",
    "\n",
    "        ### TEST LOOP ###\n",
    "        print(f\"Now testing policy...\")\n",
    "        test(env_test, policy, device, config)\n",
    "\n",
    "\n",
    "def test(env_test, policy, device, config):\n",
    "    obs, _ = env_test.reset()\n",
    "\n",
    "    # stack frames together to introduce temporal information\n",
    "    state_deque = deque()\n",
    "    for _ in range(config.stack_size):\n",
    "        state_deque.append(frame_to_tensor(obs))\n",
    "    state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "    policy.eval()\n",
    "    assert not policy.policy_net.training and not policy.value_net.training, \"Policy should be in evaluation mode here\"\n",
    "    \n",
    "    global global_step\n",
    "    global_step -= config.iteration_timesteps\n",
    "    episode_steps = 0\n",
    "    cum_reward = 0\n",
    "\n",
    "    for step in tqdm(range(config.iteration_timesteps)):\n",
    "\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        action = policy.act(state)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env_test.step(action)\n",
    "\n",
    "        global_step += 1\n",
    "        episode_steps += 1\n",
    "        cum_reward += reward\n",
    "\n",
    "        # udpate state to become next state using the new observation\n",
    "        state_deque.popleft()\n",
    "        state_deque.append(frame_to_tensor(next_obs))\n",
    "        state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            wandb.log({\"test/episodic_reward\": cum_reward, \n",
    "                    \"test/episode_length\": episode_steps,\n",
    "                    \"play/step\": global_step})\n",
    "            \n",
    "            episode_steps = 0\n",
    "            cum_reward = 0\n",
    "                \n",
    "            if step < config.iteration_timesteps - 1:\n",
    "                # reset env and initial obs\n",
    "                obs, _ = env_test.reset()\n",
    "\n",
    "                state_deque = deque()\n",
    "                for _ in range(config.stack_size):\n",
    "                    state_deque.append(frame_to_tensor(obs))\n",
    "                state = torch.concatenate(list(state_deque), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmerliedoardo\u001b[0m (\u001b[33mteamedo\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\merli/.netrc\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\merli\\OneDrive\\Desktop\\Github repos\\procgen-PPO\\wandb\\run-20240627_180843-jzkjy8wg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/teamedo/ppo-procgen/runs/jzkjy8wg' target=\"_blank\">coinrun_500_hard</a></strong> to <a href='https://wandb.ai/teamedo/ppo-procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/teamedo/ppo-procgen' target=\"_blank\">https://wandb.ai/teamedo/ppo-procgen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/teamedo/ppo-procgen/runs/jzkjy8wg' target=\"_blank\">https://wandb.ai/teamedo/ppo-procgen/runs/jzkjy8wg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy network has 113303 parameters.\n",
      "Value network has 109705 parameters.\n",
      "Total parameters: 223008.\n",
      "Using cpu device\n",
      "===============Iteration 1===============\n",
      "Playing 1024 steps...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]d:\\Users\\edo\\envs\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "100%|██████████| 1024/1024 [01:03<00:00, 16.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1024 transitions, starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:35<00:00, 31.93s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done!\n",
      "Now testing policy...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]d:\\Users\\edo\\envs\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "100%|██████████| 1024/1024 [00:28<00:00, 36.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Iteration 2===============\n",
      "Playing 1024 steps...\n",
      "Warning: early reset ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:48<00:00, 21.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1024 transitions, starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:18<00:00, 26.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done!\n",
      "Now testing policy...\n",
      "Warning: early reset ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:26<00:00, 38.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Iteration 3===============\n",
      "Playing 1024 steps...\n",
      "Warning: early reset ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:49<00:00, 20.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1024 transitions, starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:20<00:00, 26.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done!\n",
      "Now testing policy...\n",
      "Warning: early reset ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:27<00:00, 37.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===============Iteration 4===============\n",
      "Playing 1024 steps...\n",
      "Warning: early reset ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:51<00:00, 20.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1024 transitions, starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [01:23<00:00, 27.95s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training done!\n",
      "Now testing policy...\n",
      "Warning: early reset ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1024/1024 [00:28<00:00, 36.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving best model to wandb...\n",
      "Saved successfully!\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>play/episode_length</td><td>██▁▁▃</td></tr><tr><td>play/episodic_reward</td><td>█▁▁▁▁</td></tr><tr><td>play/step</td><td>▁▂▄▄▅▅▅▆█▇█</td></tr><tr><td>test/episode_length</td><td>██▃▂▁▁</td></tr><tr><td>test/episodic_reward</td><td>▁▁█▁██</td></tr><tr><td>train/batch</td><td>▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇███</td></tr><tr><td>train/entropy</td><td>███████▇▅▄▅▅▄▆▆▆▅▃▁▁▃▅▆▆▆▆▇▇▇▆▆▆▆▆▆▆▆▆▆</td></tr><tr><td>train/kl_div</td><td>▁▂█▂▃▇▅▄▅▁▁▁</td></tr><tr><td>train/loss_pi</td><td>▆▆▆▆▆▆▆▄█▂▆▆▆▆▄▄▄▃▁▇▇█▅▄▅▅▄█▆▆▆▅▅▆▅▅▆▆▆</td></tr><tr><td>train/loss_v</td><td>▂▅▃█▁▅▁▂▁▆▂▂▂▂▂▂▁▁▂▂▁▁▁▁▁▁▁▁▁▂▁▁▁▁▁▁▁▁▁</td></tr><tr><td>train/lr_policy</td><td>██████████▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁</td></tr><tr><td>train/lr_value</td><td>██████████▇▇▇▆▆▆▅▅▅▅▄▄▄▄▄▄▃▃▃▂▂▂▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>play/episode_length</td><td>312</td></tr><tr><td>play/episodic_reward</td><td>0.0</td></tr><tr><td>play/step</td><td>3450</td></tr><tr><td>test/episode_length</td><td>214</td></tr><tr><td>test/episodic_reward</td><td>10.0</td></tr><tr><td>train/batch</td><td>192</td></tr><tr><td>train/entropy</td><td>2.7029</td></tr><tr><td>train/kl_div</td><td>0.00033</td></tr><tr><td>train/loss_pi</td><td>-0.00071</td></tr><tr><td>train/loss_v</td><td>0.07606</td></tr><tr><td>train/lr_policy</td><td>1e-05</td></tr><tr><td>train/lr_value</td><td>1e-05</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">coinrun_500_hard</strong> at: <a href='https://wandb.ai/teamedo/ppo-procgen/runs/jzkjy8wg' target=\"_blank\">https://wandb.ai/teamedo/ppo-procgen/runs/jzkjy8wg</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240627_180843-jzkjy8wg\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "global_batch = 0\n",
    "global_step = 0\n",
    "\n",
    "### CONFIGURATION ###\n",
    "TOT_TIMESTEPS = int(2**21)  # approx 2M\n",
    "ITER_TIMESTEPS = 1024\n",
    "NUM_ITERATIONS = TOT_TIMESTEPS // ITER_TIMESTEPS\n",
    "DIFFICULTY = \"hard\"\n",
    "CONFIG = {\n",
    "    # Game\n",
    "    \"game\": \"coinrun\",\n",
    "    \"num_levels\": 200 if DIFFICULTY == \"easy\" else 500,\n",
    "    \"seed\": 6,\n",
    "    \"difficulty\": DIFFICULTY,\n",
    "    \"backgrounds\": False,\n",
    "    \"stack_size\": 2,\n",
    "\n",
    "    # Timesteps and iterations\n",
    "    \"tot_timesteps\": TOT_TIMESTEPS,\n",
    "    \"iteration_timesteps\": ITER_TIMESTEPS,\n",
    "    \"num_iterations\": NUM_ITERATIONS,\n",
    "\n",
    "    # Network architecture\n",
    "    \"batch_norm\": False,\n",
    "\n",
    "    # Training params\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 128,\n",
    "    \"lr_policy_network\": 5e-4,\n",
    "    \"lr_value_network\": 5e-4,\n",
    "    \"kl_limit\": 0.03,\n",
    "\n",
    "    # PPO params\n",
    "    \"gamma\": 0.999,\n",
    "    \"lambda_\": 0.95,\n",
    "    \"eps_clip\": 0.2,\n",
    "    \"entropy_bonus\": 0.01,\n",
    "    \"v_target\": \"TD-lambda\",  # \"TD-lambda\" (for advantage + value) or \"MC\" (for cumulative reward)\n",
    "    \"normalize_v_targets\": False,\n",
    "\n",
    "    # Logging\n",
    "    \"log_frequency\": 10,\n",
    "    \"log_video\": False,\n",
    "    \"episode_video_frequency\": 100,\n",
    "}\n",
    "\n",
    "\n",
    "### WANDB ###\n",
    "wandb.login()\n",
    "wandb.init(project=\"ppo-procgen\", name=f\"{CONFIG['game']}_{CONFIG['num_levels']}_{CONFIG['difficulty']}\", config=CONFIG)\n",
    "config = wandb.config\n",
    "\n",
    "wandb.define_metric(\"play/step\")\n",
    "wandb.define_metric(\"train/batch\")\n",
    "\n",
    "wandb.define_metric(\"play/episodic_reward\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"play/episode_length\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"train/loss_pi\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/loss_v\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/entropy\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/lr_policy\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/lr_value\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"test/episodic_reward\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"test/episode_length\", step_metric=\"play/step\")\n",
    "\n",
    "\n",
    "### CREATE ENVIRONMENTS ###\n",
    "env_train = gym.make(\n",
    "    f\"procgen:procgen-{config.game}-v0\",\n",
    "    num_levels=config.num_levels,\n",
    "    start_level=config.seed,\n",
    "    distribution_mode=config.difficulty,\n",
    "    use_backgrounds=config.backgrounds,\n",
    "    render_mode='rgb_array',\n",
    "    apply_api_compatibility=True,\n",
    "    rand_seed=config.seed\n",
    ")\n",
    "\n",
    "if config.log_video:\n",
    "    env_train = RecorderWrapper(env_train, config.episode_video_frequency)\n",
    "\n",
    "env_test = gym.make(\n",
    "    f\"procgen:procgen-{config.game}-v0\",\n",
    "    num_levels=0,\n",
    "    start_level=config.seed,\n",
    "    distribution_mode=config.difficulty,\n",
    "    use_backgrounds=config.backgrounds,\n",
    "    render_mode='rgb_array',\n",
    "    apply_api_compatibility=True,\n",
    "    rand_seed=config.seed\n",
    ")\n",
    "\n",
    "seed_everything(config.seed)\n",
    "\n",
    "### CREATE PPO AGENTS AND OPTIMIZERS ###\n",
    "policy = PPO(env_train, config)\n",
    "policy_old = copy.deepcopy(policy)\n",
    "\n",
    "print(f\"Policy network has {sum(p.numel() for p in policy.policy_net.parameters())} parameters.\")\n",
    "print(f\"Value network has {sum(p.numel() for p in policy.value_net.parameters())} parameters.\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in policy.policy_net.parameters()) + sum(p.numel() for p in policy.value_net.parameters())}.\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "policy.to(device)\n",
    "policy_old.to(device)\n",
    "\n",
    "optimizer_policy = torch.optim.Adam(policy.policy_net.parameters(), lr=config.lr_policy_network)\n",
    "scheduler_policy = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_policy, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "optimizer_value = torch.optim.Adam(policy.value_net.parameters(), lr=config.lr_value_network)\n",
    "scheduler_value = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_value, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "### MAIN ###\n",
    "play_and_train(env_train, env_test, policy, policy_old, optimizer_policy, optimizer_value, device, config, scheduler_policy=scheduler_policy, scheduler_value=scheduler_value)\n",
    "\n",
    "### SAVE MODEL ###\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "print(\"Saving best model to wandb...\")\n",
    "save_path = f\"models/{config.game}_{config.difficulty}.pt\"\n",
    "torch.save(policy.state_dict(), save_path)\n",
    "# upload to wandb\n",
    "artifact = wandb.Artifact(f\"model_{config.game}_{config.difficulty}\", type='model')\n",
    "artifact.add_file(save_path)\n",
    "wandb.log_artifact(artifact)\n",
    "print(f\"Saved successfully at {save_path}!\")\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
