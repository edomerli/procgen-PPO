{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook useful to train using GPUs on Kaggle/Colab, contains the same code of the repo, just reformatted for imports between files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install procgen\n",
    "# !pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import gym \n",
    "import gym.wrappers\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"Seed all sources of randomness for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionsDataset(Dataset):\n",
    "    def __init__(self, transitions, transform=None, normalize_v_targets=False, v_mu=None, v_std=None):\n",
    "        self.transitions = transitions\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.normalize_v_targets = normalize_v_targets\n",
    "        if normalize_v_targets:\n",
    "            self.v_mu = v_mu\n",
    "            self.v_std = v_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transitions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        state_t = self.transitions[idx]['s_t']\n",
    "        action_t = self.transitions[idx]['a_t']\n",
    "        advantage_t = self.transitions[idx]['A_t']\n",
    "        v_target_t = self.transitions[idx]['v_target_t']\n",
    "\n",
    "        if self.transform:\n",
    "            state_t = self.transform(state_t)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            v_target_t = (v_target_t - self.v_mu) / max(self.v_std, 1e-6)\n",
    "\n",
    "        return state_t, action_t, advantage_t, v_target_t.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, batch_norm):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        if batch_norm:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ImpalaNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_actions, batch_norm):\n",
    "        super(ImpalaNetwork, self).__init__()\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.stems = nn.ModuleList()\n",
    "        self.res_blocks1 = nn.ModuleList()\n",
    "        self.res_blocks2 = nn.ModuleList()\n",
    "\n",
    "        hidden_channels = [16, 32, 32]\n",
    "\n",
    "        for out_channels in hidden_channels:\n",
    "\n",
    "            # Don't use batch_norm in the first layer as it should go after MaxPool2d, \n",
    "            # but it's already present in the successive ConvBlock\n",
    "            self.stems.append(torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=\"same\"),\n",
    "                torch.nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ))\n",
    "\n",
    "            self.res_blocks1.append(torch.nn.Sequential(\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "            ))\n",
    "\n",
    "            self.res_blocks2.append(torch.nn.Sequential(\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "            ))\n",
    "\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.fc = torch.nn.Linear(32 * 7 * 7, out_features=256)\n",
    "\n",
    "        self.out = torch.nn.Linear(256, num_actions)\n",
    "\n",
    "        \n",
    "        if num_actions > 1:\n",
    "            # policy network initialization\n",
    "            nn.init.orthogonal_(self.fc.weight, gain=0.01)\n",
    "            nn.init.constant_(self.fc.bias, 0)\n",
    "        else:\n",
    "            # value network initialization\n",
    "            nn.init.orthogonal_(self.out.weight, gain=1)\n",
    "            nn.init.constant_(self.out.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for stem, res_block1, res_block2 in zip(self.stems, self.res_blocks1, self.res_blocks2):\n",
    "            x = stem(x)\n",
    "            x = res_block1(x) + x\n",
    "            x = res_block2(x) + x\n",
    "\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        if self.num_actions > 1:\n",
    "            logits = self.out(x)\n",
    "            output = torch.distributions.Categorical(logits=logits)\n",
    "        else:\n",
    "            output = self.out(x).squeeze()\n",
    "\n",
    "        return output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, config):\n",
    "        self.policy_net = ImpalaNetwork(config.stack_size * 3, env.action_space.n, config.batch_norm)\n",
    "        self.value_net = ImpalaNetwork(config.stack_size * 3, 1, config.batch_norm)\n",
    "\n",
    "        self.normalize_v_targets = config.normalize_v_targets\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            self.value_mean = 0\n",
    "            self.value_std = 1\n",
    "            self.values_count = 0\n",
    "\n",
    "    def act(self, state):\n",
    "        dist, value = self.actions_dist_and_v(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action.item(), value.item()\n",
    "    \n",
    "    def actions_dist_and_v(self, state):\n",
    "        dist = self.policy_net(state)\n",
    "        value = self.value_net(state)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            # denormalize value\n",
    "            value = value * max(self.value_std, 1e-6) + self.value_mean\n",
    "\n",
    "        return dist, value\n",
    "      \n",
    "    def to(self, device):\n",
    "        self.policy_net.to(device)\n",
    "        self.value_net.to(device)\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "\n",
    "    def update_v_target_stats(self, v_targets):\n",
    "        self.value_mean = (self.value_mean * self.values_count + v_targets.mean() * len(v_targets)) / (self.values_count + len(v_targets) + 1e-6)\n",
    "        self.value_std = (self.value_std * self.values_count + v_targets.std() * len(v_targets)) / (self.values_count + len(v_targets) + 1e-6)\n",
    "        self.values_count += len(v_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecorderWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, episode_frequency_rec):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.episode_frequency_rec = episode_frequency_rec\n",
    "\n",
    "        self.episode_counter = 1\n",
    "        self.recording = False if episode_frequency_rec > 1 else True\n",
    "        self.frames = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        if self.recording:\n",
    "            self.frames.append(np.moveaxis(obs, -1, 0))\n",
    "            \n",
    "            if terminated:\n",
    "                self.save_video()\n",
    "                self.recording = False\n",
    "                self.frames = []\n",
    "        \n",
    "        if terminated:\n",
    "            self.episode_counter += 1\n",
    "            if self.episode_counter % self.episode_frequency_rec == 0:\n",
    "                self.recording = True\n",
    "            \n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def save_video(self):\n",
    "        global global_step\n",
    "        wandb.log({\"video\": wandb.Video(np.array(self.frames), caption=f\"step: {global_step} - episode: {self.episode_counter}\", fps=30, format=\"mp4\")})\n",
    "\n",
    "    def close(self):\n",
    "        super().close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, policy_old, train_dataloader, optimizer_policy, optimizer_value, device, config, scheduler_policy=None, scheduler_value=None):\n",
    "\n",
    "    global global_batch\n",
    "\n",
    "    policy.train()\n",
    "    policy_old.eval()\n",
    "    assert policy_old.policy_net.training == False and policy_old.value_net.training == False, \"Old policy should be in evaluation mode here\"\n",
    "    assert policy.policy_net.training == True and policy.value_net.training == True, \"Policy should be in training mode here\"\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for batch, (states, actions, advantages, value_targets) in enumerate(train_dataloader):\n",
    "            # normalize advantages between 0 and 1\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            states = states.to(device)\n",
    "            actions = actions.to(device)\n",
    "            advantages = advantages.to(device)\n",
    "            value_targets = value_targets.to(device)\n",
    "            \n",
    "            dists, values = policy.actions_dist_and_v(states)\n",
    "            old_dists, _ = policy_old.actions_dist_and_v(states)\n",
    "\n",
    "            log_probs = dists.log_prob(actions)\n",
    "            old_log_probs = old_dists.log_prob(actions)\n",
    "\n",
    "            # Equivalent of doing exp(log_probs) / exp(old_log_probs) \n",
    "            # but avoids overflows and division by (potentially if underflown) zero, breaking loss function\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "            # clipped surrogate loss\n",
    "            l_clips = -torch.min(ratios * advantages, torch.clip(ratios, 1-config.eps_clip, 1+config.eps_clip) * advantages)\n",
    "            loss_pi = torch.mean(l_clips)\n",
    "            loss_entropy = dists.entropy().mean()\n",
    "            loss_policy = loss_pi - config.entropy_bonus * loss_entropy\n",
    "\n",
    "            # mse loss\n",
    "            loss_value = torch.nn.functional.mse_loss(values, value_targets)\n",
    "\n",
    "            # with two different optimizers\n",
    "            loss_policy.backward()\n",
    "            optimizer_policy.step()\n",
    "            optimizer_policy.zero_grad()\n",
    "\n",
    "            loss_value.backward()\n",
    "            optimizer_value.step()\n",
    "            optimizer_value.zero_grad()\n",
    "\n",
    "            if global_batch % config.log_frequency == 0:\n",
    "                wandb.log({\"train/loss_pi\": loss_pi, \n",
    "                           \"train/loss_v\": loss_value,\n",
    "                           \"train/entropy\": loss_entropy,\n",
    "                           \"train/lr_policy\": optimizer_policy.param_groups[0]['lr'],\n",
    "                           \"train/lr_value\": optimizer_value.param_groups[0]['lr'],\n",
    "                           \"train/batch\": global_batch})\n",
    "            \n",
    "            global_batch += 1\n",
    "        \n",
    "        if scheduler_policy is not None:\n",
    "            scheduler_policy.step()\n",
    "        if scheduler_value is not None:\n",
    "            scheduler_value.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # KL divergence between old and new policy for early stopping\n",
    "            kl_div = torch.distributions.kl.kl_divergence(dists, old_dists).mean().item()\n",
    "            wandb.log({\"train/kl_div\": kl_div, \"train/batch\": global_batch})\n",
    "            if kl_div > config.kl_limit:\n",
    "                print(f\"Early stopping at epoch {epoch} due to KL divergence {round(kl_div, 4)} > {config.kl_limit}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame_to_tensor = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "def compute_advantages(values, rewards, gamma, lambda_):\n",
    "    # GAE estimator\n",
    "    deltas = np.array(rewards) + gamma * np.array(values[1:]) - np.array(values[:-1])\n",
    "    advantages = [deltas[-1]] \n",
    "\n",
    "    for t in range(len(deltas)-2, -1, -1):\n",
    "        advantage_t = deltas[t] + gamma * lambda_ * advantages[-1]\n",
    "        advantages.append(advantage_t)\n",
    "\n",
    "    advantages = advantages[::-1]\n",
    "    return advantages\n",
    "\n",
    "def compute_value_targets(advantages, values, rewards, config):\n",
    "    value_targets = []\n",
    "    if config.v_target == \"TD-lambda\":\n",
    "        for t in range(len(advantages)):\n",
    "            value_targets.append(advantages[t] + values[t])\n",
    "    elif config.v_target == \"MC\":\n",
    "        value_targets.append(rewards[-1])\n",
    "        for t in range(len(rewards)-2, -1, -1):\n",
    "            value_targets.append(rewards[t] + config.gamma * value_targets[-1])\n",
    "        value_targets = value_targets[::-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value target type {config.v_target}, choose between 'TD-lambda' and 'MC'.\")\n",
    "    return value_targets\n",
    "\n",
    "\n",
    "def play_and_train(env, policy, policy_old, optimizer_policy, optimizer_value, device, config, **kwargs):  \n",
    "\n",
    "    global global_step\n",
    "\n",
    "    for iteration in range(config.num_iterations):\n",
    "        print(f\"===============Iteration {iteration+1}===============\")\n",
    "\n",
    "        transitions = []\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        # stack frames together to introduce temporal information\n",
    "        state_deque = deque()\n",
    "        for _ in range(config.stack_size):\n",
    "            state_deque.append(frame_to_tensor(obs))\n",
    "\n",
    "        state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "        trajectory = {\n",
    "            'states': [state],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'values': [],\n",
    "        }\n",
    "\n",
    "        policy.eval()\n",
    "\n",
    "        for step in tqdm(range(config.iteration_timesteps)):\n",
    "            assert not policy.policy_net.training and not policy.value_net.training, \"Policy should be in evaluation mode here\"\n",
    "\n",
    "            state = state.unsqueeze(0).to(device)\n",
    "            action, value = policy.act(state)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            truncated = truncated or step == config.iteration_timesteps - 1\n",
    "\n",
    "            # update step count\n",
    "            global_step += 1\n",
    "\n",
    "            # collect transition info in trajectory\n",
    "            trajectory['values'].append(value)\n",
    "\n",
    "            trajectory['actions'].append(action)\n",
    "            trajectory['rewards'].append(reward)\n",
    "\n",
    "            # udpate state to become next state using the new observation\n",
    "            state_deque.popleft()\n",
    "            state_deque.append(frame_to_tensor(next_obs))\n",
    "            state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "            trajectory['states'].append(state)\n",
    "\n",
    "\n",
    "            if terminated or truncated:\n",
    "                # see terminated vs truncated API at https://farama.org/Gymnasium-Terminated-Truncated-Step-API\n",
    "                if terminated:\n",
    "                    # final value is 0 if the episode terminated, i.e. reached a final state\n",
    "                    trajectory['values'].append(0)\n",
    "                else:\n",
    "                    # bootstrap if the episode was truncated, i.e. didn't reach a final state\n",
    "                    state = state.unsqueeze(0).to(device)\n",
    "                    _, value = policy.act(state)\n",
    "                    trajectory['values'].append(value)\n",
    "                \n",
    "                assert len(trajectory['states']) >= 2, \"Trajectory must have at least 2 states to compute advantages.\"\n",
    "                assert len(trajectory['states']) == len(trajectory['actions']) + 1 , \"Trajectory must have one more state than actions.\"\n",
    "                advantages = compute_advantages(trajectory['values'], trajectory['rewards'], config.gamma, config.lambda_)\n",
    "\n",
    "                value_targets = compute_value_targets(advantages, trajectory['values'], trajectory['rewards'], config)\n",
    "\n",
    "                if config.normalize_v_targets:\n",
    "                    policy.update_v_target_stats(np.array(value_targets))\n",
    "\n",
    "\n",
    "                # convert trajectory into list of transitions\n",
    "                for t in range(len(trajectory['states'])-1):    # -1 because advantages already encode the value of state t+1\n",
    "                    transitions.append({\n",
    "                        's_t': trajectory['states'][t],\n",
    "                        'a_t': trajectory['actions'][t],\n",
    "                        'A_t': advantages[t],\n",
    "                        'v_target_t': value_targets[t],\n",
    "                    })\n",
    "\n",
    "                # log and update episodes count only if episode terminated\n",
    "                if terminated:\n",
    "                    wandb.log({\"play/episodic_reward\": sum(trajectory['rewards']), \n",
    "                            \"play/episode_length\": len(trajectory['states'])-1,\n",
    "                            \"play/step\": global_step})\n",
    "                \n",
    "                if step < config.iteration_timesteps - 1:\n",
    "                    # reset env and trajectory\n",
    "                    obs, _ = env.reset()\n",
    "\n",
    "                    state_deque = deque()\n",
    "                    for _ in range(config.stack_size):\n",
    "                        state_deque.append(frame_to_tensor(obs))\n",
    "\n",
    "                    state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "                    trajectory = {\n",
    "                        'states': [state],\n",
    "                        'actions': [],\n",
    "                        'rewards': [],\n",
    "                        'values': [],\n",
    "                    }\n",
    "\n",
    "\n",
    "        # end of play loop\n",
    "        if config.normalize_v_targets:\n",
    "            dataset = TransitionsDataset(transitions, normalize_v_targets=True, v_mu=policy.value_mean, v_std=policy.value_std)\n",
    "        else:\n",
    "            dataset = TransitionsDataset(transitions)\n",
    "        train_dataloader = DataLoader(dataset, \n",
    "                                    batch_size=config.batch_size, \n",
    "                                    shuffle=True)\n",
    "\n",
    "        print(f\"Collected {len(transitions)} transitions, starting training...\")\n",
    "\n",
    "        # update policy\n",
    "        train(policy, policy_old, train_dataloader, optimizer_policy, optimizer_value, device, config, **kwargs)\n",
    "        print(\"Training done!\")\n",
    "\n",
    "        del policy_old\n",
    "        policy_old = copy.deepcopy(policy)\n",
    "        policy_old.to(device)\n",
    "\n",
    "\n",
    "def test(env, policy, device, config):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # stack frames together to introduce temporal information\n",
    "    state_deque = deque()\n",
    "    for _ in range(config.stack_size):\n",
    "        state_deque.append(frame_to_tensor(obs))\n",
    "    state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "    policy.eval()\n",
    "    assert not policy.policy_net.training and not policy.value_net.training, \"Policy should be in evaluation mode here\"\n",
    "    \n",
    "    episode_steps = 0\n",
    "    cum_reward = 0\n",
    "\n",
    "    for step in tqdm(range(config.tot_timesteps)):\n",
    "\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        action, _ = policy.act(state)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        episode_steps += 1\n",
    "        cum_reward += reward\n",
    "\n",
    "        # udpate state to become next state using the new observation\n",
    "        state_deque.popleft()\n",
    "        state_deque.append(frame_to_tensor(next_obs))\n",
    "        state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            wandb.log({\"test/episodic_reward\": cum_reward, \n",
    "                    \"test/episode_length\": episode_steps,\n",
    "                    \"test/step\": step})\n",
    "            \n",
    "            episode_steps = 0\n",
    "            cum_reward = 0\n",
    "                \n",
    "            if step < config.tot_timesteps - 1:\n",
    "                # reset env and initial obs\n",
    "                obs, _ = env.reset()\n",
    "\n",
    "                state_deque = deque()\n",
    "                for _ in range(config.stack_size):\n",
    "                    state_deque.append(frame_to_tensor(obs))\n",
    "                state = torch.concatenate(list(state_deque), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:wa1gn4fg) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMailboxError\u001b[0m                              Traceback (most recent call last)",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2211\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[1;34m(self, exit_code)\u001b[0m\n\u001b[0;32m   2210\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 2211\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2212\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m ki:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2451\u001b[0m, in \u001b[0;36mRun._on_finish\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2446\u001b[0m \u001b[38;5;66;03m# this message is confusing, we should remove it\u001b[39;00m\n\u001b[0;32m   2447\u001b[0m \u001b[38;5;66;03m# self._footer_exit_status_info(\u001b[39;00m\n\u001b[0;32m   2448\u001b[0m \u001b[38;5;66;03m#     self._exit_code, settings=self._settings, printer=self._printer\u001b[39;00m\n\u001b[0;32m   2449\u001b[0m \u001b[38;5;66;03m# )\u001b[39;00m\n\u001b[1;32m-> 2451\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[43mexit_handle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_on_progress_exit\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2453\u001b[0m poll_exit_handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39minterface\u001b[38;5;241m.\u001b[39mdeliver_poll_exit()\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:298\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_probe \u001b[38;5;129;01mand\u001b[39;00m probe_handle:\n\u001b[1;32m--> 298\u001b[0m     \u001b[43mon_probe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprobe_handle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m on_progress \u001b[38;5;129;01mand\u001b[39;00m progress_handle:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2415\u001b[0m, in \u001b[0;36mRun._on_probe_exit\u001b[1;34m(self, probe_handle)\u001b[0m\n\u001b[0;32m   2414\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m handle:\n\u001b[1;32m-> 2415\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mhandle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelease\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m   2416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m result:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\lib\\mailbox.py:281\u001b[0m, in \u001b[0;36mMailboxHandle.wait\u001b[1;34m(self, timeout, on_probe, on_progress, release, cancel)\u001b[0m\n\u001b[0;32m    280\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_interface\u001b[38;5;241m.\u001b[39m_transport_keepalive_failed():\n\u001b[1;32m--> 281\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m MailboxError(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransport failed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    283\u001b[0m found, abandoned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_slot\u001b[38;5;241m.\u001b[39m_get_and_clear(timeout\u001b[38;5;241m=\u001b[39mwait_timeout)\n",
      "\u001b[1;31mMailboxError\u001b[0m: transport failed",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mConnectionResetError\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1166\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1165\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1166\u001b[0m     run \u001b[38;5;241m=\u001b[39m \u001b[43mwi\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1167\u001b[0m     except_exit \u001b[38;5;241m=\u001b[39m wi\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39m_except_exit\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:599\u001b[0m, in \u001b[0;36m_WandbInit.init\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    595\u001b[0m     ipython\u001b[38;5;241m.\u001b[39mdisplay_html(\n\u001b[0;32m    596\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinishing last run (ID:\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlatest_run\u001b[38;5;241m.\u001b[39m_run_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) before initializing another...\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    597\u001b[0m     )\n\u001b[1;32m--> 599\u001b[0m \u001b[43mlatest_run\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfinish\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    601\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m jupyter \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39msilent:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:420\u001b[0m, in \u001b[0;36m_run_decorator._noop.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    418\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mDummy()\n\u001b[1;32m--> 420\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:361\u001b[0m, in \u001b[0;36m_run_decorator._attach.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_is_attaching \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 361\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:1953\u001b[0m, in \u001b[0;36mRun.finish\u001b[1;34m(self, exit_code, quiet)\u001b[0m\n\u001b[0;32m   1944\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Mark a run as finished, and finish uploading all data.\u001b[39;00m\n\u001b[0;32m   1945\u001b[0m \n\u001b[0;32m   1946\u001b[0m \u001b[38;5;124;03mThis is used when creating multiple runs in the same process. We automatically\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1951\u001b[0m \u001b[38;5;124;03m    quiet: Set to true to minimize log output\u001b[39;00m\n\u001b[0;32m   1952\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1953\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_finish\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquiet\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:1968\u001b[0m, in \u001b[0;36mRun._finish\u001b[1;34m(self, exit_code, quiet)\u001b[0m\n\u001b[0;32m   1966\u001b[0m         hook\u001b[38;5;241m.\u001b[39mcall()\n\u001b[1;32m-> 1968\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_atexit_cleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexit_code\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexit_code\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1969\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_wl\u001b[38;5;241m.\u001b[39m_global_run_stack) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_run.py:2222\u001b[0m, in \u001b[0;36mRun._atexit_cleanup\u001b[1;34m(self, exit_code)\u001b[0m\n\u001b[0;32m   2221\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_console_stop()\n\u001b[1;32m-> 2222\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcleanup\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2223\u001b[0m logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProblem finishing run\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\backend\\backend.py:232\u001b[0m, in \u001b[0;36mBackend.cleanup\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    231\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minterface:\n\u001b[1;32m--> 232\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minterface\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwandb_process:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:531\u001b[0m, in \u001b[0;36mInterfaceShared.join\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mjoin\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 531\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    533\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_router:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\interface\\interface.py:703\u001b[0m, in \u001b[0;36mInterfaceBase.join\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    702\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m--> 703\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate_shutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:428\u001b[0m, in \u001b[0;36mInterfaceShared._communicate_shutdown\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    427\u001b[0m record \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_record(request\u001b[38;5;241m=\u001b[39mrequest)\n\u001b[1;32m--> 428\u001b[0m _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_shared.py:294\u001b[0m, in \u001b[0;36mInterfaceShared._communicate\u001b[1;34m(self, rec, timeout, local)\u001b[0m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_communicate\u001b[39m(\n\u001b[0;32m    292\u001b[0m     \u001b[38;5;28mself\u001b[39m, rec: pb\u001b[38;5;241m.\u001b[39mRecord, timeout: Optional[\u001b[38;5;28mint\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m, local: Optional[\u001b[38;5;28mbool\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    293\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Optional[pb\u001b[38;5;241m.\u001b[39mResult]:\n\u001b[1;32m--> 294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_communicate_async\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(timeout\u001b[38;5;241m=\u001b[39mtimeout)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\interface\\interface_sock.py:60\u001b[0m, in \u001b[0;36mInterfaceSock._communicate_async\u001b[1;34m(self, rec, local)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe wandb backend process has shutdown\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 60\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_router\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_and_receive\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlocal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m future\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\interface\\router.py:94\u001b[0m, in \u001b[0;36mMessageRouter.send_and_receive\u001b[1;34m(self, rec, local)\u001b[0m\n\u001b[0;32m     92\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pending_reqs[rec\u001b[38;5;241m.\u001b[39muuid] \u001b[38;5;241m=\u001b[39m future\n\u001b[1;32m---> 94\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrec\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m future\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\interface\\router_sock.py:36\u001b[0m, in \u001b[0;36mMessageSockRouter._send_message\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_send_message\u001b[39m(\u001b[38;5;28mself\u001b[39m, record: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpb.Record\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 36\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_record_communicate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecord\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:216\u001b[0m, in \u001b[0;36mSockClient.send_record_communicate\u001b[1;34m(self, record)\u001b[0m\n\u001b[0;32m    215\u001b[0m server_req\u001b[38;5;241m.\u001b[39mrecord_communicate\u001b[38;5;241m.\u001b[39mCopyFrom(record)\n\u001b[1;32m--> 216\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_server_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mserver_req\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:155\u001b[0m, in \u001b[0;36mSockClient.send_server_request\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    154\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msend_server_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, msg: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 155\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_send_message\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:152\u001b[0m, in \u001b[0;36mSockClient._send_message\u001b[1;34m(self, msg)\u001b[0m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m--> 152\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sendall_with_error_handle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mheader\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\lib\\sock_client.py:130\u001b[0m, in \u001b[0;36mSockClient._sendall_with_error_handle\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 130\u001b[0m     sent \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;66;03m# sent equal to 0 indicates a closed socket\u001b[39;00m\n",
      "\u001b[1;31mConnectionResetError\u001b[0m: [WinError 10054] Connessione in corso interrotta forzatamente dall'host remoto",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 49\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m### WANDB ###\u001b[39;00m\n\u001b[0;32m     48\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlogin()\n\u001b[1;32m---> 49\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproject\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo-procgen\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mgame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnum_levels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mCONFIG\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdifficulty\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCONFIG\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m config \u001b[38;5;241m=\u001b[39m wandb\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m     52\u001b[0m wandb\u001b[38;5;241m.\u001b[39mdefine_metric(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplay/step\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1189\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1187\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m logger\n\u001b[0;32m   1188\u001b[0m     logger\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minterrupted\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39me)\n\u001b[1;32m-> 1189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m   1190\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1191\u001b[0m     error_seen \u001b[38;5;241m=\u001b[39m e\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\sdk\\wandb_init.py:1170\u001b[0m, in \u001b[0;36minit\u001b[1;34m(job_type, dir, config, project, entity, reinit, tags, group, name, notes, magic, config_exclude_keys, config_include_keys, anonymous, mode, allow_val_change, resume, force, tensorboard, sync_tensorboard, monitor_gym, save_code, id, settings)\u001b[0m\n\u001b[0;32m   1168\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m, \u001b[38;5;167;01mException\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m   1169\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m):\n\u001b[1;32m-> 1170\u001b[0m         \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sentry\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexception\u001b[49m\u001b[43m(\u001b[49m\u001b[43me\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1171\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\n\u001b[0;32m   1172\u001b[0m         wandb\u001b[38;5;241m.\u001b[39mwandb_agent\u001b[38;5;241m.\u001b[39m_is_running() \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m)\n\u001b[0;32m   1173\u001b[0m     ):\n\u001b[0;32m   1174\u001b[0m         getcaller()\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\analytics\\sentry.py:43\u001b[0m, in \u001b[0;36m_safe_noop.<locals>.wrapper\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m     41\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 43\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     45\u001b[0m     \u001b[38;5;66;03m# do not call self.exception here to avoid infinite recursion\u001b[39;00m\n\u001b[0;32m     46\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m func\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexception\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\wandb\\analytics\\sentry.py:140\u001b[0m, in \u001b[0;36mSentry.exception\u001b[1;34m(self, exc, handled, status)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmark_session(status\u001b[38;5;241m=\u001b[39mstatus)\n\u001b[0;32m    139\u001b[0m client, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhub\u001b[38;5;241m.\u001b[39m_stack[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m--> 140\u001b[0m \u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\sentry_sdk\\client.py:675\u001b[0m, in \u001b[0;36m_Client.flush\u001b[1;34m(self, timeout, callback)\u001b[0m\n\u001b[0;32m    673\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_aggregator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    674\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetrics_aggregator\u001b[38;5;241m.\u001b[39mflush()\n\u001b[1;32m--> 675\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtransport\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\sentry_sdk\\transport.py:553\u001b[0m, in \u001b[0;36mHttpTransport.flush\u001b[1;34m(self, timeout, callback)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    552\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_worker\u001b[38;5;241m.\u001b[39msubmit(\u001b[38;5;28;01mlambda\u001b[39;00m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flush_client_reports(force\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\n\u001b[1;32m--> 553\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_worker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\sentry_sdk\\worker.py:101\u001b[0m, in \u001b[0;36mBackgroundWorker.flush\u001b[1;34m(self, timeout, callback)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[0;32m    100\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_alive \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m--> 101\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_wait_flush\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbackground worker flushed\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\sentry_sdk\\worker.py:117\u001b[0m, in \u001b[0;36mBackgroundWorker._wait_flush\u001b[1;34m(self, timeout, callback)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    115\u001b[0m     callback(pending, timeout)\n\u001b[1;32m--> 117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_timed_queue_join\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minitial_timeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m    118\u001b[0m     pending \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39mqsize() \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    119\u001b[0m     logger\u001b[38;5;241m.\u001b[39merror(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflush timed out, dropped \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m events\u001b[39m\u001b[38;5;124m\"\u001b[39m, pending)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\sentry_sdk\\worker.py:56\u001b[0m, in \u001b[0;36mBackgroundWorker._timed_queue_join\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m delay \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m     55\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 56\u001b[0m         \u001b[43mqueue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mall_tasks_done\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdelay\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\threading.py:324\u001b[0m, in \u001b[0;36mCondition.wait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    322\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    323\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 324\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    325\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    326\u001b[0m         gotit \u001b[38;5;241m=\u001b[39m waiter\u001b[38;5;241m.\u001b[39macquire(\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "global_batch = 0\n",
    "global_step = 0\n",
    "\n",
    "### CONFIGURATION ###\n",
    "TOT_TIMESTEPS = int(2**18)  #int(2**20)  # approx 1M\n",
    "ITER_TIMESTEPS = 1024\n",
    "NUM_ITERATIONS = TOT_TIMESTEPS // ITER_TIMESTEPS\n",
    "CONFIG = {\n",
    "    # Game\n",
    "    \"game\": \"coinrun\",\n",
    "    \"num_levels\": 200,\n",
    "    \"seed\": 6,\n",
    "    \"difficulty\": \"easy\",\n",
    "    \"backgrounds\": False,\n",
    "    \"stack_size\": 4,\n",
    "\n",
    "    # Timesteps and iterations\n",
    "    \"tot_timesteps\": TOT_TIMESTEPS,\n",
    "    \"iteration_timesteps\": ITER_TIMESTEPS,\n",
    "    \"num_iterations\": NUM_ITERATIONS,\n",
    "\n",
    "    # Network architecture\n",
    "    \"batch_norm\": True,\n",
    "\n",
    "    # Training params\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr_policy_network\": 5e-4,\n",
    "    \"lr_value_network\": 5e-4,\n",
    "    \"kl_limit\": 0.015,\n",
    "\n",
    "    # PPO params\n",
    "    \"gamma\": 0.999,\n",
    "    \"lambda_\": 0.95,\n",
    "    \"eps_clip\": 0.2,\n",
    "    \"entropy_bonus\": 0.01,\n",
    "    \"v_target\": \"TD-lambda\",  # \"TD-lambda\" (for advantage + value) or \"MC\" (for cumulative reward)\n",
    "    \"normalize_v_targets\": True,\n",
    "\n",
    "    # Logging\n",
    "    \"log_frequency\": 5,\n",
    "    \"log_video\": True,\n",
    "    \"episode_video_frequency\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "### WANDB ###\n",
    "wandb.login()\n",
    "wandb.init(project=\"ppo-procgen\", name=f\"{CONFIG['game']}_{CONFIG['num_levels']}_{CONFIG['difficulty']}\", config=CONFIG)\n",
    "config = wandb.config\n",
    "\n",
    "wandb.define_metric(\"play/step\")\n",
    "wandb.define_metric(\"train/batch\")\n",
    "wandb.define_metric(\"test/step\")\n",
    "\n",
    "wandb.define_metric(\"play/episodic_reward\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"play/episode_length\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"train/loss_pi\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/loss_v\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/entropy\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/lr_policy\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/lr_value\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"test/episodic_reward\", step_metric=\"test/step\")\n",
    "wandb.define_metric(\"test/episode_length\", step_metric=\"test/step\")\n",
    "\n",
    "\n",
    "### PLAY AND TRAIN PHASE ###\n",
    "env = gym.make(\n",
    "    f\"procgen:procgen-{config.game}-v0\",\n",
    "    num_levels=config.num_levels,\n",
    "    start_level=config.seed,\n",
    "    distribution_mode=config.difficulty,\n",
    "    use_backgrounds=config.backgrounds,\n",
    "    render_mode='rgb_array',\n",
    "    apply_api_compatibility=True,\n",
    "    rand_seed=config.seed\n",
    ")\n",
    "\n",
    "if config.log_video:\n",
    "    env = RecorderWrapper(env, config.episode_video_frequency)\n",
    "\n",
    "seed_everything(config.seed)\n",
    "\n",
    "### CREATE PPO AGENTS AND OPTIMIZERS ###\n",
    "policy = PPO(env, config)\n",
    "policy_old = copy.deepcopy(policy)\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in policy.policy_net.parameters()) + sum(p.numel() for p in policy.value_net.parameters())} total parameters.\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "policy.to(device)\n",
    "policy_old.to(device)\n",
    "\n",
    "optimizer_policy = torch.optim.Adam(policy.policy_net.parameters(), lr=config.lr_policy_network)\n",
    "scheduler_policy = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_policy, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "optimizer_value = torch.optim.Adam(policy.value_net.parameters(), lr=config.lr_value_network)\n",
    "scheduler_value = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_value, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "play_and_train(env, policy, policy_old, optimizer_policy, optimizer_value, device, config, scheduler_policy=scheduler_policy, scheduler_value=scheduler_value)\n",
    "\n",
    "\n",
    "### TEST PHASE ###\n",
    "env_test = gym.make(\n",
    "    f\"procgen:procgen-{config.game}-v0\",\n",
    "    num_levels=0,\n",
    "    start_level=config.seed,\n",
    "    distribution_mode=config.difficulty,\n",
    "    use_backgrounds=config.backgrounds,\n",
    "    render_mode='rgb_array',\n",
    "    apply_api_compatibility=True,\n",
    "    rand_seed=config.seed\n",
    ")\n",
    "\n",
    "test(env_test, policy, device, config)\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
