{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook useful to train using GPUs on Kaggle/Colab, contains the same code of the repo, just reformatted for imports between files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install procgen\n",
    "# !pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import gym \n",
    "import gym.wrappers\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_batch = 0\n",
    "global_step = 0\n",
    "\n",
    "def seed_everything(seed):\n",
    "    \"\"\"Seed all sources of randomness for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionsDataset(Dataset):\n",
    "    def __init__(self, transitions, transform=None, normalize_v_targets=False, v_mu=None, v_std=None):\n",
    "        self.transitions = transitions\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.normalize_v_targets = normalize_v_targets\n",
    "        if normalize_v_targets:\n",
    "            self.v_mu = v_mu\n",
    "            self.v_std = v_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transitions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        state_t = self.transitions[idx]['s_t']\n",
    "        action_t = self.transitions[idx]['a_t']\n",
    "        advantage_t = self.transitions[idx]['A_t']\n",
    "        v_target_t = self.transitions[idx]['v_target_t']\n",
    "\n",
    "        if self.transform:\n",
    "            state_t = self.transform(state_t)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            v_target_t = (v_target_t - self.v_mu) / max(self.v_std, 1e-6)\n",
    "\n",
    "        return state_t, action_t, advantage_t, v_target_t.astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, batch_norm):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        if batch_norm:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ImpalaNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_actions, batch_norm):\n",
    "        super(ImpalaNetwork, self).__init__()\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.stems = nn.ModuleList()\n",
    "        self.res_blocks1 = nn.ModuleList()\n",
    "        self.res_blocks2 = nn.ModuleList()\n",
    "\n",
    "        hidden_channels = [16, 32, 32]\n",
    "\n",
    "        for out_channels in hidden_channels:\n",
    "\n",
    "            # Don't use batch_norm in the first layer as it should go after MaxPool2d, \n",
    "            # but it's already present in the successive ConvBlock\n",
    "            self.stems.append(torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=\"same\"),\n",
    "                torch.nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ))\n",
    "\n",
    "            self.res_blocks1.append(torch.nn.Sequential(\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "            ))\n",
    "\n",
    "            self.res_blocks2.append(torch.nn.Sequential(\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "            ))\n",
    "\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.fc = torch.nn.Linear(32 * 7 * 7, out_features=256)\n",
    "\n",
    "        self.out = torch.nn.Linear(256, num_actions)\n",
    "\n",
    "        \n",
    "        if num_actions > 1:\n",
    "            # policy network initialization\n",
    "            nn.init.orthogonal_(self.fc.weight, gain=0.01)\n",
    "            nn.init.constant_(self.fc.bias, 0)\n",
    "        else:\n",
    "            # value network initialization\n",
    "            nn.init.orthogonal_(self.out.weight, gain=1)\n",
    "            nn.init.constant_(self.out.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for stem, res_block1, res_block2 in zip(self.stems, self.res_blocks1, self.res_blocks2):\n",
    "            x = stem(x)\n",
    "            x = res_block1(x) + x\n",
    "            x = res_block2(x) + x\n",
    "\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        if self.num_actions > 1:\n",
    "            logits = self.out(x)\n",
    "            output = torch.distributions.Categorical(logits=logits)\n",
    "        else:\n",
    "            output = self.out(x).squeeze()\n",
    "\n",
    "        return output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, config):\n",
    "        self.policy_net = ImpalaNetwork(config.stack_size * 3, env.action_space.n, config.batch_norm)\n",
    "        self.value_net = ImpalaNetwork(config.stack_size * 3, 1, config.batch_norm)\n",
    "\n",
    "        self.normalize_v_targets = config.normalize_v_targets\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            self.value_mean = 0\n",
    "            self.value_std = 1\n",
    "            self.values_count = 0\n",
    "\n",
    "    def act(self, state):\n",
    "        dist, value = self.actions_dist_and_v(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action.item(), value.item()\n",
    "    \n",
    "    def actions_dist_and_v(self, state):\n",
    "        dist = self.policy_net(state)\n",
    "        value = self.value_net(state)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            # denormalize value\n",
    "            value = value * max(self.value_std, 1e-6) + self.value_mean\n",
    "\n",
    "        return dist, value\n",
    "      \n",
    "    def to(self, device):\n",
    "        self.policy_net.to(device)\n",
    "        self.value_net.to(device)\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "\n",
    "    def update_v_target_stats(self, v_targets):\n",
    "        self.value_mean = (self.value_mean * self.values_count + v_targets.mean() * len(v_targets)) / (self.values_count + len(v_targets) + 1e-6)\n",
    "        self.value_std = (self.value_std * self.values_count + v_targets.std() * len(v_targets)) / (self.values_count + len(v_targets) + 1e-6)\n",
    "        self.values_count += len(v_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecorderWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, episode_frequency_rec):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.episode_frequency_rec = episode_frequency_rec\n",
    "\n",
    "        self.episode_counter = 1\n",
    "        self.recording = False if episode_frequency_rec > 1 else True\n",
    "        self.frames = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        if self.recording:\n",
    "            self.frames.append(np.moveaxis(obs, -1, 0))\n",
    "            \n",
    "            if terminated:\n",
    "                self.save_video()\n",
    "                self.recording = False\n",
    "                self.frames = []\n",
    "        \n",
    "        if terminated:\n",
    "            self.episode_counter += 1\n",
    "            if self.episode_counter % self.episode_frequency_rec == 0:\n",
    "                self.recording = True\n",
    "            \n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def save_video(self):\n",
    "        global global_step\n",
    "        wandb.log({\"video\": wandb.Video(np.array(self.frames), caption=f\"step: {global_step} - episode: {self.episode_counter}\", fps=30, format=\"mp4\")})\n",
    "\n",
    "    def close(self):\n",
    "        super().close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, policy_old, train_dataloader, optimizer_policy, optimizer_value, device, config, scheduler_policy=None, scheduler_value=None):\n",
    "\n",
    "    global global_batch\n",
    "\n",
    "    policy.train()\n",
    "    policy_old.eval()\n",
    "    assert policy_old.policy_net.training == False and policy_old.value_net.training == False, \"Old policy should be in evaluation mode here\"\n",
    "    assert policy.policy_net.training == True and policy.value_net.training == True, \"Policy should be in training mode here\"\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for batch, (states, actions, advantages, value_targets) in enumerate(train_dataloader):\n",
    "            # normalize advantages between 0 and 1\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            states = states.to(device)\n",
    "            actions = actions.to(device)\n",
    "            advantages = advantages.to(device)\n",
    "            value_targets = value_targets.to(device)\n",
    "            \n",
    "            dists, values = policy.actions_dist_and_v(states)\n",
    "            old_dists, _ = policy_old.actions_dist_and_v(states)\n",
    "\n",
    "            log_probs = dists.log_prob(actions)\n",
    "            old_log_probs = old_dists.log_prob(actions)\n",
    "\n",
    "            # Equivalent of doing exp(log_probs) / exp(old_log_probs) \n",
    "            # but avoids overflows and division by (potentially if underflown) zero, breaking loss function\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "            # clipped surrogate loss\n",
    "            l_clips = -torch.min(ratios * advantages, torch.clip(ratios, 1-config.eps_clip, 1+config.eps_clip) * advantages)\n",
    "            loss_pi = torch.mean(l_clips)\n",
    "            loss_entropy = dists.entropy().mean()\n",
    "            loss_policy = loss_pi - config.entropy_bonus * loss_entropy\n",
    "\n",
    "            # mse loss\n",
    "            loss_value = torch.nn.functional.mse_loss(values, value_targets)\n",
    "\n",
    "            # with two different optimizers\n",
    "            loss_policy.backward()\n",
    "            optimizer_policy.step()\n",
    "            optimizer_policy.zero_grad()\n",
    "\n",
    "            loss_value.backward()\n",
    "            optimizer_value.step()\n",
    "            optimizer_value.zero_grad()\n",
    "\n",
    "            if global_batch % config.log_frequency == 0:\n",
    "                wandb.log({\"train/loss_pi\": loss_pi, \n",
    "                           \"train/loss_v\": loss_value,\n",
    "                           \"train/entropy\": loss_entropy,\n",
    "                           \"train/lr_policy\": optimizer_policy.param_groups[0]['lr'],\n",
    "                           \"train/lr_value\": optimizer_value.param_groups[0]['lr'],\n",
    "                           \"train/batch\": global_batch})\n",
    "            \n",
    "            global_batch += 1\n",
    "        \n",
    "        if scheduler_policy is not None:\n",
    "            scheduler_policy.step()\n",
    "        if scheduler_value is not None:\n",
    "            scheduler_value.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # KL divergence between old and new policy for early stopping\n",
    "            kl_div = torch.distributions.kl.kl_divergence(dists, old_dists).mean().item()\n",
    "            wandb.log({\"train/kl_div\": kl_div, \"train/batch\": global_batch})\n",
    "            if kl_div > config.kl_limit:\n",
    "                print(f\"Early stopping at epoch {epoch} due to KL divergence {round(kl_div, 4)} > {config.kl_limit}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame_to_tensor = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "def compute_advantages(values, rewards, gamma, lambda_):\n",
    "    # GAE estimator\n",
    "    deltas = np.array(rewards) + gamma * np.array(values[1:]) - np.array(values[:-1])\n",
    "    advantages = [deltas[-1]] \n",
    "\n",
    "    for t in range(len(deltas)-2, -1, -1):\n",
    "        advantage_t = deltas[t] + gamma * lambda_ * advantages[-1]\n",
    "        advantages.append(advantage_t)\n",
    "\n",
    "    advantages = advantages[::-1]\n",
    "    return advantages\n",
    "\n",
    "def compute_value_targets(advantages, values, rewards, config):\n",
    "    value_targets = []\n",
    "    if config.v_target == \"TD-lambda\":\n",
    "        for t in range(len(advantages)):\n",
    "            value_targets.append(advantages[t] + values[t])\n",
    "    elif config.v_target == \"MC\":\n",
    "        value_targets.append(rewards[-1])\n",
    "        for t in range(len(rewards)-2, -1, -1):\n",
    "            value_targets.append(rewards[t] + config.gamma * value_targets[-1])\n",
    "        value_targets = value_targets[::-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value target type {config.v_target}, choose between 'TD-lambda' and 'MC'.\")\n",
    "    return value_targets\n",
    "\n",
    "\n",
    "def play_and_train(env, policy, policy_old, optimizer_policy, optimizer_value, device, config, **kwargs):  \n",
    "\n",
    "    global global_step\n",
    "    # obs, _ = env.reset()   # TODO: aggiungi solo se metti step sotto invece di reset\n",
    "\n",
    "    for iteration in range(config.num_iterations):\n",
    "        print(f\"===============Iteration {iteration+1}===============\")\n",
    "\n",
    "        transitions = []\n",
    "\n",
    "        obs, _ = env.reset()    # TODO: prova a mettere env.step qui\n",
    "\n",
    "        # stack frames together to introduce temporal information\n",
    "        state_deque = deque()\n",
    "        for _ in range(config.stack_size):\n",
    "            state_deque.append(frame_to_tensor(obs))\n",
    "\n",
    "        state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "        trajectory = {\n",
    "            'states': [state],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'values': [],\n",
    "        }\n",
    "\n",
    "        policy.eval()\n",
    "\n",
    "        for step in tqdm(range(config.iteration_timesteps)):\n",
    "            assert not policy.policy_net.training and not policy.value_net.training, \"Policy should be in evaluation mode here\"\n",
    "\n",
    "            state = state.unsqueeze(0).to(device)\n",
    "            action, value = policy.act(state)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            truncated = truncated or step == config.iteration_timesteps - 1\n",
    "\n",
    "            # update step count\n",
    "            global_step += 1\n",
    "\n",
    "            # collect transition info in trajectory\n",
    "            trajectory['values'].append(value)\n",
    "\n",
    "            trajectory['actions'].append(action)\n",
    "            trajectory['rewards'].append(reward)\n",
    "\n",
    "            # udpate state to become next state using the new observation\n",
    "            state_deque.popleft()\n",
    "            state_deque.append(frame_to_tensor(next_obs))\n",
    "            state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "            trajectory['states'].append(state)\n",
    "\n",
    "\n",
    "            if terminated or truncated:\n",
    "                # see terminated vs truncated API at https://farama.org/Gymnasium-Terminated-Truncated-Step-API\n",
    "                if terminated:\n",
    "                    # final value is 0 if the episode terminated, i.e. reached a final state\n",
    "                    trajectory['values'].append(0)\n",
    "                else:\n",
    "                    # bootstrap if the episode was truncated, i.e. didn't reach a final state\n",
    "                    state = state.unsqueeze(0).to(device)\n",
    "                    _, value = policy.act(state)\n",
    "                    trajectory['values'].append(value)\n",
    "                \n",
    "                assert len(trajectory['states']) >= 2, \"Trajectory must have at least 2 states to compute advantages.\"\n",
    "                assert len(trajectory['states']) == len(trajectory['actions']) + 1 , \"Trajectory must have one more state than actions.\"\n",
    "                advantages = compute_advantages(trajectory['values'], trajectory['rewards'], config.gamma, config.lambda_)\n",
    "\n",
    "                value_targets = compute_value_targets(advantages, trajectory['values'], trajectory['rewards'], config)\n",
    "\n",
    "                if config.normalize_v_targets:\n",
    "                    policy.update_v_target_stats(np.array(value_targets))\n",
    "\n",
    "\n",
    "                # convert trajectory into list of transitions\n",
    "                for t in range(len(trajectory['states'])-1):    # -1 because advantages already encode the value of state t+1\n",
    "                    transitions.append({\n",
    "                        's_t': trajectory['states'][t],\n",
    "                        'a_t': trajectory['actions'][t],\n",
    "                        'A_t': advantages[t],\n",
    "                        'v_target_t': value_targets[t],\n",
    "                    })\n",
    "\n",
    "                # log and update episodes count only if episode terminated\n",
    "                if terminated:\n",
    "                    wandb.log({\"play/episodic_reward\": sum(trajectory['rewards']), \n",
    "                            \"play/episode_length\": len(trajectory['states'])-1,\n",
    "                            \"play/step\": global_step})\n",
    "                \n",
    "                if step < config.iteration_timesteps - 1:\n",
    "                    # reset env and trajectory\n",
    "                    obs, _ = env.reset()\n",
    "\n",
    "                    state_deque = deque()\n",
    "                    for _ in range(config.stack_size):\n",
    "                        state_deque.append(frame_to_tensor(obs))\n",
    "\n",
    "                    state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "                    trajectory = {\n",
    "                        'states': [state],\n",
    "                        'actions': [],\n",
    "                        'rewards': [],\n",
    "                        'values': [],\n",
    "                    }\n",
    "\n",
    "\n",
    "        # end of play loop\n",
    "        if config.normalize_v_targets:\n",
    "            dataset = TransitionsDataset(transitions, normalize_v_targets=True, v_mu=policy.value_mean, v_std=policy.value_std)\n",
    "        else:\n",
    "            dataset = TransitionsDataset(transitions)\n",
    "        train_dataloader = DataLoader(dataset, \n",
    "                                    batch_size=config.batch_size, \n",
    "                                    shuffle=True)\n",
    "\n",
    "        print(f\"Collected {len(transitions)} transitions, starting training...\")\n",
    "\n",
    "        # update policy\n",
    "        train(policy, policy_old, train_dataloader, optimizer_policy, optimizer_value, device, config, **kwargs)\n",
    "        print(\"Training done!\")\n",
    "\n",
    "        del policy_old\n",
    "        policy_old = copy.deepcopy(policy)\n",
    "        policy_old.to(device)\n",
    "\n",
    "\n",
    "def test(env, policy, device, config):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # stack frames together to introduce temporal information\n",
    "    state_deque = deque()\n",
    "    for _ in range(config.stack_size):\n",
    "        state_deque.append(frame_to_tensor(obs))\n",
    "    state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "    policy.eval()\n",
    "    assert not policy.policy_net.training and not policy.value_net.training, \"Policy should be in evaluation mode here\"\n",
    "    \n",
    "    episode_steps = 0\n",
    "    cum_reward = 0\n",
    "\n",
    "    for step in tqdm(range(config.tot_timesteps)):\n",
    "\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        action, _ = policy.act(state)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        episode_steps += 1\n",
    "        cum_reward += reward\n",
    "\n",
    "        # udpate state to become next state using the new observation\n",
    "        state_deque.popleft()\n",
    "        state_deque.append(frame_to_tensor(next_obs))\n",
    "        state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            wandb.log({\"test/episodic_reward\": cum_reward, \n",
    "                    \"test/episode_length\": episode_steps,\n",
    "                    \"test/step\": step})\n",
    "            \n",
    "            episode_steps = 0\n",
    "            cum_reward = 0\n",
    "                \n",
    "            if step < config.tot_timesteps - 1:\n",
    "                # reset env and initial obs\n",
    "                obs, _ = env.reset()\n",
    "\n",
    "                state_deque = deque()\n",
    "                for _ in range(config.stack_size):\n",
    "                    state_deque.append(frame_to_tensor(obs))\n",
    "                state = torch.concatenate(list(state_deque), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:anhc5jnt) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">coinrun_200_easy</strong> at: <a href='https://wandb.ai/teamedo/ppo-procgen/runs/anhc5jnt' target=\"_blank\">https://wandb.ai/teamedo/ppo-procgen/runs/anhc5jnt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>.\\wandb\\run-20240625_104147-anhc5jnt\\logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:anhc5jnt). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.3 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>c:\\Users\\merli\\OneDrive\\Desktop\\Github repos\\procgen-PPO\\wandb\\run-20240625_104349-wa1gn4fg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/teamedo/ppo-procgen/runs/wa1gn4fg' target=\"_blank\">coinrun_200_easy</a></strong> to <a href='https://wandb.ai/teamedo/ppo-procgen' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/teamedo/ppo-procgen' target=\"_blank\">https://wandb.ai/teamedo/ppo-procgen</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/teamedo/ppo-procgen/runs/wa1gn4fg' target=\"_blank\">https://wandb.ai/teamedo/ppo-procgen/runs/wa1gn4fg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model has 1006512 total parameters.\n",
      "Using cpu device\n",
      "===============Iteration 1===============\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1024 [00:00<?, ?it/s]d:\\Users\\edo\\envs\\rl\\lib\\site-packages\\gym\\utils\\passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n",
      "  if not isinstance(terminated, (bool, np.bool8)):\n",
      "100%|██████████| 1024/1024 [00:31<00:00, 32.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collected 1024 transitions, starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [01:25<00:42, 42.99s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Early stopping at epoch 2 due to KL divergence 0.025 > 0.015\n",
      "Training done!\n",
      "===============Iteration 2===============\n",
      "Warning: early reset ignored\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 22%|██▏       | 224/1024 [00:08<00:31, 25.04it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 99\u001b[0m\n\u001b[0;32m     96\u001b[0m optimizer_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdam(policy\u001b[38;5;241m.\u001b[39mvalue_net\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mlr_value_network)\n\u001b[0;32m     97\u001b[0m scheduler_value \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mCosineAnnealingLR(optimizer_value, T_max\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_iterations\u001b[38;5;241m*\u001b[39mconfig\u001b[38;5;241m.\u001b[39mepochs, eta_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-6\u001b[39m)\n\u001b[1;32m---> 99\u001b[0m \u001b[43mplay_and_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpolicy_old\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_policy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_policy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscheduler_value\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;66;03m### TEST PHASE ###\u001b[39;00m\n\u001b[0;32m    103\u001b[0m env_test \u001b[38;5;241m=\u001b[39m gym\u001b[38;5;241m.\u001b[39mmake(\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprocgen:procgen-\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39mgame\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m-v0\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    105\u001b[0m     num_levels\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    111\u001b[0m     rand_seed\u001b[38;5;241m=\u001b[39mconfig\u001b[38;5;241m.\u001b[39mseed\n\u001b[0;32m    112\u001b[0m )\n",
      "Cell \u001b[1;32mIn[16], line 62\u001b[0m, in \u001b[0;36mplay_and_train\u001b[1;34m(env, policy, policy_old, optimizer_policy, optimizer_value, device, config, **kwargs)\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m policy\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m policy\u001b[38;5;241m.\u001b[39mvalue_net\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPolicy should be in evaluation mode here\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     61\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 62\u001b[0m action, value \u001b[38;5;241m=\u001b[39m \u001b[43mpolicy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     64\u001b[0m next_obs, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n\u001b[0;32m     65\u001b[0m truncated \u001b[38;5;241m=\u001b[39m truncated \u001b[38;5;129;01mor\u001b[39;00m step \u001b[38;5;241m==\u001b[39m config\u001b[38;5;241m.\u001b[39miteration_timesteps \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "Cell \u001b[1;32mIn[13], line 102\u001b[0m, in \u001b[0;36mPPO.act\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m--> 102\u001b[0m     dist, value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions_dist_and_v\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m     action \u001b[38;5;241m=\u001b[39m dist\u001b[38;5;241m.\u001b[39msample()\n\u001b[0;32m    105\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\u001b[38;5;241m.\u001b[39mitem(), value\u001b[38;5;241m.\u001b[39mitem()\n",
      "Cell \u001b[1;32mIn[13], line 108\u001b[0m, in \u001b[0;36mPPO.actions_dist_and_v\u001b[1;34m(self, state)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mactions_dist_and_v\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[1;32m--> 108\u001b[0m     dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpolicy_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    109\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_net(state)\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnormalize_v_targets:\n\u001b[0;32m    112\u001b[0m         \u001b[38;5;66;03m# denormalize value\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[13], line 71\u001b[0m, in \u001b[0;36mImpalaNetwork.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     70\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m stem, res_block1, res_block2 \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstems, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_blocks1, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mres_blocks2):\n\u001b[1;32m---> 71\u001b[0m         x \u001b[38;5;241m=\u001b[39m \u001b[43mstem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     72\u001b[0m         x \u001b[38;5;241m=\u001b[39m res_block1(x) \u001b[38;5;241m+\u001b[39m x\n\u001b[0;32m     73\u001b[0m         x \u001b[38;5;241m=\u001b[39m res_block2(x) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:164\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 164\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    165\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    166\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\_jit_internal.py:497\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 497\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Users\\edo\\envs\\rl\\lib\\site-packages\\torch\\nn\\functional.py:796\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    794\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    795\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 796\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "### CONFIGURATION ###\n",
    "TOT_TIMESTEPS = int(2**18)  #int(2**20)  # approx 1M\n",
    "ITER_TIMESTEPS = 1024\n",
    "NUM_ITERATIONS = TOT_TIMESTEPS // ITER_TIMESTEPS\n",
    "CONFIG = {\n",
    "    # Game\n",
    "    \"game\": \"coinrun\",\n",
    "    \"num_levels\": 200,\n",
    "    \"seed\": 6,\n",
    "    \"difficulty\": \"easy\",\n",
    "    \"backgrounds\": False,\n",
    "    \"stack_size\": 4,\n",
    "\n",
    "    # Timesteps and iterations\n",
    "    \"tot_timesteps\": TOT_TIMESTEPS,\n",
    "    \"iteration_timesteps\": ITER_TIMESTEPS,\n",
    "    \"num_iterations\": NUM_ITERATIONS,\n",
    "\n",
    "    # Network architecture\n",
    "    \"batch_norm\": True,\n",
    "\n",
    "    # Training params\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr_policy_network\": 5e-4,\n",
    "    \"lr_value_network\": 5e-4,\n",
    "    \"kl_limit\": 0.015,\n",
    "\n",
    "    # PPO params\n",
    "    \"gamma\": 0.999,\n",
    "    \"lambda_\": 0.95,\n",
    "    \"eps_clip\": 0.2,\n",
    "    \"entropy_bonus\": 0.01,\n",
    "    \"v_target\": \"TD-lambda\",  # \"TD-lambda\" (for advantage + value) or \"MC\" (for cumulative reward)\n",
    "    \"normalize_v_targets\": True,\n",
    "\n",
    "    # Logging\n",
    "    \"log_frequency\": 5,\n",
    "    \"log_video\": True,\n",
    "    \"episode_video_frequency\": 5,\n",
    "}\n",
    "\n",
    "\n",
    "### WANDB ###\n",
    "wandb.login()\n",
    "wandb.init(project=\"ppo-procgen\", name=f\"{CONFIG['game']}_{CONFIG['num_levels']}_{CONFIG['difficulty']}\", config=CONFIG)\n",
    "config = wandb.config\n",
    "\n",
    "wandb.define_metric(\"play/step\")\n",
    "wandb.define_metric(\"train/batch\")\n",
    "wandb.define_metric(\"test/step\")\n",
    "\n",
    "wandb.define_metric(\"play/episodic_reward\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"play/episode_length\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"train/loss_pi\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/loss_v\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/entropy\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/lr_policy\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/lr_value\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"test/episodic_reward\", step_metric=\"test/step\")\n",
    "wandb.define_metric(\"test/episode_length\", step_metric=\"test/step\")\n",
    "\n",
    "\n",
    "### PLAY AND TRAIN PHASE ###\n",
    "env = gym.make(\n",
    "    f\"procgen:procgen-{config.game}-v0\",\n",
    "    num_levels=config.num_levels,\n",
    "    start_level=config.seed,\n",
    "    distribution_mode=config.difficulty,\n",
    "    use_backgrounds=config.backgrounds,\n",
    "    render_mode='rgb_array',\n",
    "    apply_api_compatibility=True,\n",
    "    rand_seed=config.seed\n",
    ")\n",
    "\n",
    "if config.log_video:\n",
    "    env = RecorderWrapper(env, config.episode_video_frequency)\n",
    "\n",
    "seed_everything(config.seed)\n",
    "\n",
    "### CREATE PPO AGENTS AND OPTIMIZERS ###\n",
    "policy = PPO(env, config)\n",
    "policy_old = copy.deepcopy(policy)\n",
    "\n",
    "print(f\"Model has {sum(p.numel() for p in policy.policy_net.parameters()) + sum(p.numel() for p in policy.value_net.parameters())} total parameters.\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "policy.to(device)\n",
    "policy_old.to(device)\n",
    "\n",
    "optimizer_policy = torch.optim.Adam(policy.policy_net.parameters(), lr=config.lr_policy_network)\n",
    "scheduler_policy = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_policy, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "optimizer_value = torch.optim.Adam(policy.value_net.parameters(), lr=config.lr_value_network)\n",
    "scheduler_value = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_value, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "play_and_train(env, policy, policy_old, optimizer_policy, optimizer_value, device, config, scheduler_policy=scheduler_policy, scheduler_value=scheduler_value)\n",
    "\n",
    "\n",
    "### TEST PHASE ###\n",
    "env_test = gym.make(\n",
    "    f\"procgen:procgen-{config.game}-v0\",\n",
    "    num_levels=0,\n",
    "    start_level=config.seed,\n",
    "    distribution_mode=config.difficulty,\n",
    "    use_backgrounds=config.backgrounds,\n",
    "    render_mode='rgb_array',\n",
    "    apply_api_compatibility=True,\n",
    "    rand_seed=config.seed\n",
    ")\n",
    "\n",
    "test(env_test, policy, device, config)\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
