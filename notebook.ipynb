{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook useful to train using GPUs on Kaggle/Colab, contains the same code of the repo, just reformatted for imports between files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install procgen\n",
    "# !pip install moviepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "from collections import deque\n",
    "from tqdm import tqdm\n",
    "import copy\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.ops import SqueezeExcitation\n",
    "\n",
    "\n",
    "import gym \n",
    "import gym.wrappers\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    \"\"\"Seed all sources of randomness for reproducibility\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransitionsDataset(Dataset):\n",
    "    def __init__(self, transitions, transform=None, normalize_v_targets=False, v_mu=None, v_std=None):\n",
    "        self.transitions = transitions\n",
    "        self.transform = transform\n",
    "        \n",
    "        self.normalize_v_targets = normalize_v_targets\n",
    "        if normalize_v_targets:\n",
    "            self.v_mu = v_mu\n",
    "            self.v_std = v_std\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.transitions)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        state_t = self.transitions[idx]['s_t']\n",
    "        action_t = self.transitions[idx]['a_t']\n",
    "        advantage_t = self.transitions[idx]['A_t']\n",
    "        v_target_t = self.transitions[idx]['v_target_t']\n",
    "\n",
    "        if self.transform:\n",
    "            state_t = self.transform(state_t)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            v_target_t = (v_target_t - self.v_mu) / max(self.v_std, 1e-6)\n",
    "\n",
    "        return state_t, action_t, advantage_t, v_target_t.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, batch_norm):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        if batch_norm:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.BatchNorm2d(in_channels),\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            )\n",
    "        else:\n",
    "            self.layer = nn.Sequential(\n",
    "                nn.ReLU(),\n",
    "                nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, stride=stride, padding=padding),\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "class ImpalaNetwork(torch.nn.Module):\n",
    "    def __init__(self, in_channels, num_actions, batch_norm):\n",
    "        super(ImpalaNetwork, self).__init__()\n",
    "        \n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        self.stems = nn.ModuleList()\n",
    "        self.res_blocks1 = nn.ModuleList()\n",
    "        self.res_blocks2 = nn.ModuleList()\n",
    "\n",
    "        hidden_channels = [16, 32, 32]\n",
    "\n",
    "        for out_channels in hidden_channels:\n",
    "\n",
    "            # Don't use batch_norm in the first layer as it should go after MaxPool2d, \n",
    "            # but it's already present in the successive ConvBlock\n",
    "            self.stems.append(torch.nn.Sequential(\n",
    "                torch.nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=3, padding=\"same\"),\n",
    "                torch.nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ))\n",
    "\n",
    "            self.res_blocks1.append(torch.nn.Sequential(\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                SqueezeExcitation(out_channels, 4)\n",
    "            ))\n",
    "\n",
    "            self.res_blocks2.append(torch.nn.Sequential(\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                ConvBlock(in_channels=out_channels, out_channels=out_channels, kernel_size=3, stride=1, padding=\"same\", batch_norm=batch_norm),\n",
    "                SqueezeExcitation(out_channels, 4)\n",
    "            ))\n",
    "\n",
    "            in_channels = out_channels\n",
    "\n",
    "        self.global_avg_pool = torch.nn.AdaptiveAvgPool2d((1, 1))\n",
    "\n",
    "        self.fc = torch.nn.Linear(hidden_channels[-1], out_features=256)\n",
    "\n",
    "        self.out = torch.nn.Linear(256, num_actions)\n",
    "\n",
    "        \n",
    "        if num_actions > 1:\n",
    "            # policy network initialization\n",
    "            nn.init.orthogonal_(self.fc.weight, gain=0.01)\n",
    "            nn.init.constant_(self.fc.bias, 0)\n",
    "        else:\n",
    "            # value network initialization\n",
    "            nn.init.orthogonal_(self.out.weight, gain=1)\n",
    "            nn.init.constant_(self.out.bias, 0)\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        for stem, res_block1, res_block2 in zip(self.stems, self.res_blocks1, self.res_blocks2):\n",
    "            x = stem(x)\n",
    "            x = res_block1(x) + x\n",
    "            x = res_block2(x) + x\n",
    "\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        x = self.global_avg_pool(x)\n",
    "\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = self.fc(x)\n",
    "        x = nn.functional.relu(x)\n",
    "\n",
    "        if self.num_actions > 1:\n",
    "            logits = self.out(x)\n",
    "            output = torch.distributions.Categorical(logits=logits)\n",
    "        else:\n",
    "            output = self.out(x).squeeze()\n",
    "\n",
    "        return output\n",
    "\n",
    "class PPO:\n",
    "    def __init__(self, env, config):\n",
    "        self.policy_net = ImpalaNetwork(config.stack_size * 3, env.action_space.n, config.batch_norm)\n",
    "        self.value_net = ImpalaNetwork(config.stack_size * 3, 1, config.batch_norm)\n",
    "\n",
    "        self.normalize_v_targets = config.normalize_v_targets\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            self.value_mean = 0\n",
    "            self.value_std = 1\n",
    "            self.values_count = 0\n",
    "\n",
    "    # act(), value() and act_and_v() are used during play, hence a single value (.item()) is returned\n",
    "    def act(self, state):\n",
    "        dist = self.policy_net(state)\n",
    "        action = dist.sample()\n",
    "\n",
    "        return action.item()\n",
    "    \n",
    "    def value(self, state):\n",
    "        value = self.value_net(state)\n",
    "\n",
    "        if self.normalize_v_targets:\n",
    "            # denormalize value\n",
    "            value = value * max(self.value_std, 1e-6) + self.value_mean\n",
    "\n",
    "        return value.item()\n",
    "\n",
    "    def act_and_v(self, state):\n",
    "        action = self.act(state)\n",
    "        value = self.value(state)\n",
    "\n",
    "        return action, value\n",
    "    \n",
    "    # actions_dist() and actions_dist_and_v() are used during training, hence the full distributions and values are returned\n",
    "    def actions_dist(self, state):\n",
    "        return self.policy_net(state)\n",
    "    \n",
    "    def actions_dist_and_v(self, state):\n",
    "        dist = self.policy_net(state)\n",
    "        value = self.value_net(state)\n",
    "\n",
    "        return dist, value\n",
    "      \n",
    "    def to(self, device):\n",
    "        self.policy_net.to(device)\n",
    "        self.value_net.to(device)\n",
    "\n",
    "    def eval(self):\n",
    "        self.policy_net.eval()\n",
    "        self.value_net.eval()\n",
    "\n",
    "    def train(self):\n",
    "        self.policy_net.train()\n",
    "        self.value_net.train()\n",
    "\n",
    "    def update_v_target_stats(self, v_targets):\n",
    "        self.value_mean = (self.value_mean * self.values_count + v_targets.mean() * len(v_targets)) / (self.values_count + len(v_targets) + 1e-6)\n",
    "        self.value_std = (self.value_std * self.values_count + v_targets.std() * len(v_targets)) / (self.values_count + len(v_targets) + 1e-6)\n",
    "        self.values_count += len(v_targets)\n",
    "\n",
    "    def state_dict(self):\n",
    "        return self.policy_net.state_dict()\n",
    "    \n",
    "    def load_state_dict(self, state_dict):\n",
    "        self.policy_net.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecorderWrapper(gym.Wrapper):\n",
    "    def __init__(self, env, episode_frequency_rec):\n",
    "        super().__init__(env)\n",
    "        self.env = env\n",
    "        self.episode_frequency_rec = episode_frequency_rec\n",
    "\n",
    "        self.episode_counter = 1\n",
    "        self.recording = False if episode_frequency_rec > 1 else True\n",
    "        self.frames = []\n",
    "\n",
    "    def reset(self, **kwargs):\n",
    "        return self.env.reset(**kwargs)\n",
    "\n",
    "    def step(self, action):\n",
    "        obs, reward, terminated, truncated, info = self.env.step(action)\n",
    "\n",
    "        if self.recording:\n",
    "            self.frames.append(np.moveaxis(obs, -1, 0))\n",
    "            \n",
    "            if terminated:\n",
    "                self.save_video()\n",
    "                self.recording = False\n",
    "                self.frames = []\n",
    "        \n",
    "        if terminated:\n",
    "            self.episode_counter += 1\n",
    "            if self.episode_counter % self.episode_frequency_rec == 0:\n",
    "                self.recording = True\n",
    "            \n",
    "\n",
    "        return obs, reward, terminated, truncated, info\n",
    "    \n",
    "    def save_video(self):\n",
    "        global global_step\n",
    "        wandb.log({\"video\": wandb.Video(np.array(self.frames), caption=f\"step: {global_step} - episode: {self.episode_counter}\", fps=30, format=\"mp4\")})\n",
    "\n",
    "    def close(self):\n",
    "        super().close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(policy, policy_old, train_dataloader, optimizer_policy, optimizer_value, device, config, scheduler_policy=None, scheduler_value=None):\n",
    "\n",
    "    global global_batch\n",
    "\n",
    "    policy.train()\n",
    "    policy_old.eval()\n",
    "    assert policy_old.policy_net.training == False and policy_old.value_net.training == False, \"Old policy should be in evaluation mode here\"\n",
    "    assert policy.policy_net.training == True and policy.value_net.training == True, \"Policy should be in training mode here\"\n",
    "    for epoch in tqdm(range(config.epochs)):\n",
    "        for batch, (states, actions, advantages, value_targets) in enumerate(train_dataloader):\n",
    "            # normalize advantages between 0 and 1\n",
    "            advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "\n",
    "            states = states.to(device)\n",
    "            actions = actions.to(device)\n",
    "            advantages = advantages.to(device)\n",
    "            value_targets = value_targets.to(device)\n",
    "            \n",
    "            dists, values = policy.actions_dist_and_v(states)\n",
    "            old_dists = policy_old.actions_dist(states)\n",
    "\n",
    "            log_probs = dists.log_prob(actions)\n",
    "            old_log_probs = old_dists.log_prob(actions)\n",
    "\n",
    "            # Equivalent of doing exp(log_probs) / exp(old_log_probs) \n",
    "            # but avoids overflows and division by (potentially if underflown) zero, breaking loss function\n",
    "            ratios = torch.exp(log_probs - old_log_probs)\n",
    "\n",
    "            # clipped surrogate loss\n",
    "            l_clips = -torch.min(ratios * advantages, torch.clip(ratios, 1-config.eps_clip, 1+config.eps_clip) * advantages)\n",
    "            loss_pi = torch.mean(l_clips)\n",
    "            loss_entropy = dists.entropy().mean()\n",
    "            loss_policy = loss_pi - config.entropy_bonus * loss_entropy\n",
    "\n",
    "            # mse loss\n",
    "            loss_value = torch.nn.functional.mse_loss(values, value_targets)\n",
    "\n",
    "            # with two different optimizers\n",
    "            loss_policy.backward()\n",
    "            optimizer_policy.step()\n",
    "            optimizer_policy.zero_grad()\n",
    "\n",
    "            loss_value.backward()\n",
    "            optimizer_value.step()\n",
    "            optimizer_value.zero_grad()\n",
    "\n",
    "            if global_batch % config.log_frequency == 0:\n",
    "                wandb.log({\"train/loss_pi\": loss_pi, \n",
    "                           \"train/loss_v\": loss_value,\n",
    "                           \"train/entropy\": loss_entropy,\n",
    "                           \"train/lr_policy\": optimizer_policy.param_groups[0]['lr'],\n",
    "                           \"train/lr_value\": optimizer_value.param_groups[0]['lr'],\n",
    "                           \"train/batch\": global_batch})\n",
    "            \n",
    "            global_batch += 1\n",
    "        \n",
    "        if scheduler_policy is not None:\n",
    "            scheduler_policy.step()\n",
    "        if scheduler_value is not None:\n",
    "            scheduler_value.step()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # KL divergence between old and new policy for early stopping\n",
    "            kl_div = torch.distributions.kl.kl_divergence(dists, old_dists).mean().item()\n",
    "            wandb.log({\"train/kl_div\": kl_div, \"train/batch\": global_batch})\n",
    "            if kl_div > config.kl_limit:\n",
    "                print(f\"Early stopping at epoch {epoch} due to KL divergence {round(kl_div, 4)} > {config.kl_limit}\")\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "frame_to_tensor = transforms.Compose([transforms.ToPILImage(), transforms.ToTensor()])\n",
    "\n",
    "def compute_advantages(values, rewards, gamma, lambda_):\n",
    "    assert len(values) >= 2, \"Values should have at least 2 elements.\"\n",
    "    assert len(values) == len(rewards) + 1, \"Values and rewards should have the same length, with values having one more element.\"\n",
    "    # GAE estimator\n",
    "    deltas = np.array(rewards) + gamma * np.array(values[1:]) - np.array(values[:-1])\n",
    "    advantages = [deltas[-1]] \n",
    "\n",
    "    for t in range(len(deltas)-2, -1, -1):\n",
    "        advantage_t = deltas[t] + gamma * lambda_ * advantages[-1]\n",
    "        advantages.append(advantage_t)\n",
    "\n",
    "    advantages = advantages[::-1]\n",
    "    return advantages\n",
    "\n",
    "def compute_value_targets(advantages, values, rewards, config):\n",
    "    value_targets = []\n",
    "    if config.v_target == \"TD-lambda\":\n",
    "        for t in range(len(advantages)):\n",
    "            value_targets.append(advantages[t] + values[t])\n",
    "    elif config.v_target == \"MC\":\n",
    "        value_targets.append(rewards[-1])\n",
    "        for t in range(len(rewards)-2, -1, -1):\n",
    "            value_targets.append(rewards[t] + config.gamma * value_targets[-1])\n",
    "        value_targets = value_targets[::-1]\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown value target type {config.v_target}, choose between 'TD-lambda' and 'MC'.\")\n",
    "    return value_targets\n",
    "\n",
    "\n",
    "def play_and_train(env, policy, policy_old, optimizer_policy, optimizer_value, device, config, **kwargs):  \n",
    "\n",
    "    global global_step\n",
    "    \n",
    "    for iteration in range(config.num_iterations):\n",
    "        print(f\"===============Iteration {iteration+1}===============\")\n",
    "\n",
    "        transitions = []\n",
    "\n",
    "        obs, _ = env.reset()\n",
    "\n",
    "        # stack frames together to introduce temporal information\n",
    "        state_deque = deque()\n",
    "        for _ in range(config.stack_size):\n",
    "            state_deque.append(frame_to_tensor(obs))\n",
    "\n",
    "        state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "        trajectory = {\n",
    "            'states': [state],\n",
    "            'actions': [],\n",
    "            'rewards': [],\n",
    "            'values': [],\n",
    "        }\n",
    "\n",
    "        policy.eval()\n",
    "\n",
    "        for step in tqdm(range(config.iteration_timesteps)):\n",
    "            assert not policy.policy_net.training and not policy.value_net.training, \"Policy should be in evaluation mode here\"\n",
    "\n",
    "            state = state.unsqueeze(0).to(device)\n",
    "            action, value = policy.act_and_v(state)\n",
    "\n",
    "            next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "            truncated = truncated or step == config.iteration_timesteps - 1\n",
    "\n",
    "            # update step count\n",
    "            global_step += 1\n",
    "\n",
    "            # collect transition info in trajectory\n",
    "            trajectory['values'].append(value)\n",
    "\n",
    "            trajectory['actions'].append(action)\n",
    "            trajectory['rewards'].append(reward)\n",
    "\n",
    "            # udpate state to become next state using the new observation\n",
    "            state_deque.popleft()\n",
    "            state_deque.append(frame_to_tensor(next_obs))\n",
    "            state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "            trajectory['states'].append(state)\n",
    "\n",
    "\n",
    "            if terminated or truncated:\n",
    "                # see terminated vs truncated API at https://farama.org/Gymnasium-Terminated-Truncated-Step-API\n",
    "                if terminated:\n",
    "                    # final value is 0 if the episode terminated, i.e. reached a final state\n",
    "                    trajectory['values'].append(0)\n",
    "                else:\n",
    "                    # bootstrap if the episode was truncated, i.e. didn't reach a final state\n",
    "                    state = state.unsqueeze(0).to(device)\n",
    "                    value = policy.value(state)\n",
    "                    trajectory['values'].append(value)\n",
    "                \n",
    "                advantages = compute_advantages(trajectory['values'], trajectory['rewards'], config.gamma, config.lambda_)\n",
    "\n",
    "                value_targets = compute_value_targets(advantages, trajectory['values'], trajectory['rewards'], config)\n",
    "\n",
    "                if config.normalize_v_targets:\n",
    "                    policy.update_v_target_stats(np.array(value_targets))\n",
    "\n",
    "\n",
    "                # convert trajectory into list of transitions\n",
    "                for t in range(len(trajectory['states'])-1):    # -1 because advantages already encode the value of state t+1\n",
    "                    transitions.append({\n",
    "                        's_t': trajectory['states'][t],\n",
    "                        'a_t': trajectory['actions'][t],\n",
    "                        'A_t': advantages[t],\n",
    "                        'v_target_t': value_targets[t],\n",
    "                    })\n",
    "\n",
    "                # log and update episodes count only if episode terminated\n",
    "                if terminated:\n",
    "                    wandb.log({\"play/episodic_reward\": sum(trajectory['rewards']), \n",
    "                            \"play/episode_length\": len(trajectory['states'])-1,\n",
    "                            \"play/step\": global_step})\n",
    "                \n",
    "                if step < config.iteration_timesteps - 1:\n",
    "                    # reset env and trajectory\n",
    "                    obs, _ = env.reset()\n",
    "\n",
    "                    state_deque = deque()\n",
    "                    for _ in range(config.stack_size):\n",
    "                        state_deque.append(frame_to_tensor(obs))\n",
    "\n",
    "                    state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "                    trajectory = {\n",
    "                        'states': [state],\n",
    "                        'actions': [],\n",
    "                        'rewards': [],\n",
    "                        'values': [],\n",
    "                    }\n",
    "\n",
    "\n",
    "        # end of play loop\n",
    "        if config.normalize_v_targets:\n",
    "            dataset = TransitionsDataset(transitions, normalize_v_targets=True, v_mu=policy.value_mean, v_std=policy.value_std)\n",
    "        else:\n",
    "            dataset = TransitionsDataset(transitions)\n",
    "        train_dataloader = DataLoader(dataset, \n",
    "                                    batch_size=config.batch_size, \n",
    "                                    shuffle=True)\n",
    "\n",
    "        print(f\"Collected {len(transitions)} transitions, starting training...\")\n",
    "\n",
    "        # update policy\n",
    "        train(policy, policy_old, train_dataloader, optimizer_policy, optimizer_value, device, config, **kwargs)\n",
    "        print(\"Training done!\")\n",
    "\n",
    "        del policy_old\n",
    "        policy_old = copy.deepcopy(policy)\n",
    "        policy_old.to(device)\n",
    "\n",
    "\n",
    "def test(env, policy, device, config):\n",
    "    obs, _ = env.reset()\n",
    "\n",
    "    # stack frames together to introduce temporal information\n",
    "    state_deque = deque()\n",
    "    for _ in range(config.stack_size):\n",
    "        state_deque.append(frame_to_tensor(obs))\n",
    "    state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "    policy.eval()\n",
    "    assert not policy.policy_net.training and not policy.value_net.training, \"Policy should be in evaluation mode here\"\n",
    "    \n",
    "    episode_steps = 0\n",
    "    cum_reward = 0\n",
    "\n",
    "    for step in tqdm(range(config.tot_timesteps)):\n",
    "\n",
    "        state = state.unsqueeze(0).to(device)\n",
    "        action = policy.act(state)\n",
    "\n",
    "        next_obs, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        episode_steps += 1\n",
    "        cum_reward += reward\n",
    "\n",
    "        # udpate state to become next state using the new observation\n",
    "        state_deque.popleft()\n",
    "        state_deque.append(frame_to_tensor(next_obs))\n",
    "        state = torch.concatenate(list(state_deque), axis=0)\n",
    "\n",
    "        if terminated or truncated:\n",
    "            wandb.log({\"test/episodic_reward\": cum_reward, \n",
    "                    \"test/episode_length\": episode_steps,\n",
    "                    \"play/step\": step})\n",
    "            \n",
    "            episode_steps = 0\n",
    "            cum_reward = 0\n",
    "                \n",
    "            if step < config.tot_timesteps - 1:\n",
    "                # reset env and initial obs\n",
    "                obs, _ = env.reset()\n",
    "\n",
    "                state_deque = deque()\n",
    "                for _ in range(config.stack_size):\n",
    "                    state_deque.append(frame_to_tensor(obs))\n",
    "                state = torch.concatenate(list(state_deque), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "global_batch = 0\n",
    "global_step = 0\n",
    "\n",
    "### CONFIGURATION ###\n",
    "TOT_TIMESTEPS = int(2**20)  # approx 1M\n",
    "ITER_TIMESTEPS = 1024\n",
    "NUM_ITERATIONS = TOT_TIMESTEPS // ITER_TIMESTEPS\n",
    "DIFFICULTY = \"easy\"\n",
    "CONFIG = {\n",
    "    # Game\n",
    "    \"game\": \"coinrun\",\n",
    "    \"num_levels\": 200 if DIFFICULTY == \"easy\" else 500,\n",
    "    \"seed\": 6,\n",
    "    \"difficulty\": DIFFICULTY,\n",
    "    \"backgrounds\": False,\n",
    "    \"stack_size\": 4,\n",
    "\n",
    "    # Timesteps and iterations\n",
    "    \"tot_timesteps\": TOT_TIMESTEPS,\n",
    "    \"iteration_timesteps\": ITER_TIMESTEPS,\n",
    "    \"num_iterations\": NUM_ITERATIONS,\n",
    "\n",
    "    # Network architecture\n",
    "    \"batch_norm\": True,\n",
    "\n",
    "    # Training params\n",
    "    \"epochs\": 3,\n",
    "    \"batch_size\": 64,\n",
    "    \"lr_policy_network\": 5e-4,\n",
    "    \"lr_value_network\": 5e-4,\n",
    "    \"kl_limit\": 0.015,\n",
    "\n",
    "    # PPO params\n",
    "    \"gamma\": 0.999,\n",
    "    \"lambda_\": 0.95,\n",
    "    \"eps_clip\": 0.2,\n",
    "    \"entropy_bonus\": 0.01,\n",
    "    \"v_target\": \"TD-lambda\",  # \"TD-lambda\" (for advantage + value) or \"MC\" (for cumulative reward)\n",
    "    \"normalize_v_targets\": True,\n",
    "\n",
    "    # Logging\n",
    "    \"log_frequency\": 5,\n",
    "    \"log_video\": False,\n",
    "    \"episode_video_frequency\": 100,\n",
    "}\n",
    "\n",
    "\n",
    "### WANDB ###\n",
    "wandb.login(key=\"14a7d0e7554bbddd13ca1a8d45472f7a95e73ca4\")\n",
    "wandb.init(project=\"ppo-procgen\", name=f\"{CONFIG['game']}_{CONFIG['num_levels']}_{CONFIG['difficulty']}\", config=CONFIG)\n",
    "config = wandb.config\n",
    "\n",
    "wandb.define_metric(\"play/step\")\n",
    "wandb.define_metric(\"train/batch\")\n",
    "\n",
    "wandb.define_metric(\"play/episodic_reward\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"play/episode_length\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"train/loss_pi\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/loss_v\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/entropy\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/lr_policy\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"train/lr_value\", step_metric=\"train/batch\")\n",
    "wandb.define_metric(\"test/episodic_reward\", step_metric=\"play/step\")\n",
    "wandb.define_metric(\"test/episode_length\", step_metric=\"play/step\")\n",
    "\n",
    "\n",
    "### PLAY AND TRAIN PHASE ###\n",
    "env = gym.make(\n",
    "    f\"procgen:procgen-{config.game}-v0\",\n",
    "    num_levels=config.num_levels,\n",
    "    start_level=config.seed,\n",
    "    distribution_mode=config.difficulty,\n",
    "    use_backgrounds=config.backgrounds,\n",
    "    render_mode='rgb_array',\n",
    "    apply_api_compatibility=True,\n",
    "    rand_seed=config.seed\n",
    ")\n",
    "\n",
    "if config.log_video:\n",
    "    env = RecorderWrapper(env, config.episode_video_frequency)\n",
    "\n",
    "seed_everything(config.seed)\n",
    "\n",
    "### CREATE PPO AGENTS AND OPTIMIZERS ###\n",
    "policy = PPO(env, config)\n",
    "policy_old = copy.deepcopy(policy)\n",
    "\n",
    "print(f\"Policy network has {sum(p.numel() for p in policy.policy_net.parameters())} parameters.\")\n",
    "print(f\"Value network has {sum(p.numel() for p in policy.value_net.parameters())} parameters.\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in policy.policy_net.parameters()) + sum(p.numel() for p in policy.value_net.parameters())}.\")\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "policy.to(device)\n",
    "policy_old.to(device)\n",
    "\n",
    "optimizer_policy = torch.optim.Adam(policy.policy_net.parameters(), lr=config.lr_policy_network)\n",
    "# scheduler_policy = torch.optim.lr_scheduler.OneCycleLR(optimizer_policy, max_lr=config.lr_policy_network, total_steps=config.num_iterations*config.epochs, pct_start=0.1)\n",
    "scheduler_policy = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_policy, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "optimizer_value = torch.optim.Adam(policy.value_net.parameters(), lr=config.lr_value_network)\n",
    "# scheduler_value = torch.optim.lr_scheduler.OneCycleLR(optimizer_value, max_lr=config.lr_value_network, total_steps=config.num_iterations*config.epochs, pct_start=0.1)\n",
    "scheduler_value = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_value, T_max=config.num_iterations*config.epochs, eta_min=1e-6)\n",
    "\n",
    "play_and_train(env, policy, policy_old, optimizer_policy, optimizer_value, device, config, scheduler_policy=scheduler_policy, scheduler_value=scheduler_value)\n",
    "\n",
    "### SAVE MODEL ###\n",
    "if not os.path.exists(\"models\"):\n",
    "    os.makedirs(\"models\")\n",
    "\n",
    "print(\"Saving model to wandb...\")\n",
    "save_path = f\"models/{config.game}_{config.difficulty}.pt\"\n",
    "torch.save(policy.state_dict(), save_path)\n",
    "# use policy.load_state_dict(torch.load(PATH)) to load the model\n",
    "# upload to wandb\n",
    "artifact = wandb.Artifact(f\"model_{config.game}_{config.difficulty}\", type='model')\n",
    "artifact.add_file(save_path)\n",
    "wandb.log_artifact(artifact)\n",
    "print(\"Saved successfully!\")\n",
    "\n",
    "# delete file\n",
    "os.remove(save_path)\n",
    "\n",
    "\n",
    "### TEST PHASE ###\n",
    "env_test = gym.make(\n",
    "    f\"procgen:procgen-{config.game}-v0\",\n",
    "    num_levels=0,\n",
    "    start_level=config.seed,\n",
    "    distribution_mode=config.difficulty,\n",
    "    use_backgrounds=config.backgrounds,\n",
    "    render_mode='rgb_array',\n",
    "    apply_api_compatibility=True,\n",
    "    rand_seed=config.seed\n",
    ")\n",
    "\n",
    "print(\"Beginning test phase...\")\n",
    "test(env_test, policy, device, config)\n",
    "\n",
    "\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
